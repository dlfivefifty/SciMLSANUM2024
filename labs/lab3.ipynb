{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SciML SANUM2024\n",
    "# Lab 3: Reverse-mode automatic differentiation and Zygote.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the number of unknowns becomes large forward-mode automatic differentiation as\n",
    "implemented in ForwardDiff.jl becomes prohibitively expensive for computing gradients and instead we need to\n",
    "use reverse-mode automatic differentiation: this is best thought of as implementing the chain-rule\n",
    "in an automatic fashion, with a specific choice of multiplying the underlying Jacobians."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Computing gradients is important for solving optimisation problems, which is what ultimately what training a neural network\n",
    "is. Therefore we also look at solving some\n",
    "simple optimissation problems, using Optimsation.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Learning Outcomes**\n",
    "\n",
    "1. Basics of reverse-mode automatic differentiation and pullbacks.\n",
    "2. Implementation via Zygote.jl\n",
    "3. Adding custom pullbacks.\n",
    "4. Using automatic differentiation for implementing gradient descent.\n",
    "5. Solving optimisation with gradient descent and via Optimsation.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.1 Using Zygote.jl for differentiation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin with a simple demonstration of Zygote.jl, which can be thought of as a replacement for ForwardDiff.jl that\n",
    "uses reverse-mode differentiation under the hood. We can differentiate scalar functions, but unlike ForwardDiff.jl it\n",
    "overloads the `'` syntax to mean differentiation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Zygote, Test\n",
    "\n",
    "@test cos'(0.1) â‰ˆ -sin(0.1) # Differentiates cos using reverse-mode autodiff"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gradients can be computed as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f = function(x)\n",
    "    ret = zero(eltype(x))\n",
    "    for k = 1:length(x)-1\n",
    "        ret += x[k]*x[k+1]\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "x = randn(5)\n",
    "Zygote.gradient(f,x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unlike ForwardDiff.jl, the gradient returns a tuple since multiple arguments are supported in addition\n",
    "to vector inputs, eg:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x,y = 0.1, 0.2\n",
    "@test all(Zygote.gradient((x,y) -> cos(x*exp(y)), x, y) .â‰ˆ [-sin(x*exp(y))*exp(y), -sin(x*exp(y))*x*exp(y)])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now differentiating this function is not particularly faster than ForwardDiff.jl:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = randn(1000)\n",
    "@time Zygote.gradient(f, x);\n",
    "x = randn(10_000)\n",
    "@time Zygote.gradient(f, x); # roughly 200x slower"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is because not all operations are ameniable to reverse-mode differentiation as implemented in Zygote.jl.\n",
    "However, if we restrict to vectorised operations we see a dramatic improvement:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f_vec = x -> sum(x[1:end-1] .* x[2:end]) # a vectorised version of the previus function\n",
    "Zygote.gradient(f_vec, x) # compile\n",
    "@time Zygote.gradient(f_vec, x); #  1500x faster ğŸ¤©"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another catch is Zygote.jl doesn't support functions that mutate arrays. Here's an example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f! = function(x)\n",
    "    n = length(x)\n",
    "    ret = zeros(eltype(x), n)\n",
    "    for k = 1:n-1\n",
    "        ret[k] = x[k]*x[k+1] # modifies the vector ret\n",
    "    end\n",
    "    sum(ret)\n",
    "end\n",
    "\n",
    "\n",
    "x = randn(5)\n",
    "Zygote.gradient(f!,x) # errors out"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is unlike `ForwardDiff.gradient` which works fine for differentiating `f!`.\n",
    "The conclusion: Zygote.jl is much more brittle, sometimes fails outright, but when it works\n",
    "well it is _extremely_ fast. Thus when we get to neural networks it is paramount that\n",
    "we design our representation of the neural network in a way that is ameniable to reverse-mode\n",
    "automatic differentiation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Pullbacks and back-propagation for scalar functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now peek a little under-the-hood to get some intuition on how Zygote.jl is computing\n",
    "derivatives. Underlying automatic differentiation in Zygote.jl isare so-called \"pullback\"s. In the scalar\n",
    "case these are very close to the notion of a derivative. However, rather than\n",
    "the derivative being a single constant, its a linear map: eg, if the derivative\n",
    "of $f(x)$ is denoted $f'(x)$ then the pullback is a linear map\n",
    "$$\n",
    "t â†¦ f'(x)t.\n",
    "$$\n",
    "We can compute pullbacks using the `pullback` routine:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Zygote: pullback\n",
    "\n",
    "s, sin_J = pullback(sin, 0.1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`sin_J` contains the map $t â†¦ \\cos(0.1) t$. Since pullbacks support multiple arguments\n",
    "it actually returns a tuple with a single entry:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "sin_J(1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus to get out the value we use the following:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@test sin_J(1)[1] == cos(0.1)\n",
    "@test sin_J(2)[1] == 2cos(0.1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The reason its a map instead of just a scalar becomes important for the vector-valued case\n",
    "where Jacobians can often be applied to vectors much faster than creating a matrix and\n",
    "performing a matrix-vector multiplication."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pullbacks can be used for determining more complicated derivatives. Consider a composition of three functions\n",
    "$h âˆ˜ g âˆ˜ f$ where from the Chain Rule we know:\n",
    "$$\n",
    "{\\rm d} \\over {\\rm d x}[f(g(h(x))] = f'(g(h(x)) g'(h(x)) h'(x)\n",
    "$$\n",
    "Essentially we have three pullbacks: the first is the pullback of $f$ evaluated\n",
    "at $x$, the second corresponding to $g$ evaluated at $f(x)$, and the third\n",
    "corresponding to $h$ evaluated at $g(f(x))$, that is:\n",
    "$$\n",
    "\\begin{align*}\n",
    " p_1(t) &= f'(x) t  \\\\\n",
    " p_2(t) &= g'(f(x)) t  \\\\\n",
    " p_3(t) &= h'(g(f(x))t\n",
    "\\end{align*}\n",
    "$$\n",
    "Thus the derivative is given by either the _forward_ or _reverse_ composition of these functions:\n",
    "$$\n",
    " p_3(p_2(p_1(1))) = p_1(p_2(p_3(1))) = h'(g(f(x))g'(f(x))f'(x).\n",
    "$$\n",
    "The first version is called _forward-propagation_ and the second called _back-propagation_.\n",
    "Forward-propagation is a version of forward-mode automatic differentiation and is essentially equivalent to using dual number.\n",
    "We will see in the vector case that back-propagation is much more efficient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see this in action for computing the derivative of $\\cos\\sqrt{{\\rm e}^x}$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = 0.1 # point we want to differentiate\n",
    "y,pâ‚ = pullback(exp, x)\n",
    "z,pâ‚‚ = pullback(sqrt, y) # y is exp(x)\n",
    "w,pâ‚ƒ = pullback(cos, z) # z is sqrt(exp(x))\n",
    "\n",
    "@test w == cos(sqrt(exp(x)))\n",
    "\n",
    "@test pâ‚(pâ‚‚(pâ‚ƒ(1)...)...)[1] â‰ˆ pâ‚ƒ(pâ‚‚(pâ‚(1)...)...)[1] â‰ˆ -sin(sqrt(exp(x)))*exp(x)/(2sqrt(exp(x)))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see how this can lead to an approach for automatic differentiation.\n",
    "For example, consider the following function composing `sin`` over and over:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function manysin(n, x)\n",
    "    r = x\n",
    "    for k = 1:n\n",
    "        r = sin(r)\n",
    "    end\n",
    "    r\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we would need `n` pullbacks as each time `sin` is called at a different value.\n",
    "But the number of such pullbacks grows only linearly so this is acceptable. So thus\n",
    "at a high-level we can think of Zygote as running through and computing all the pullbacks:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 5\n",
    "x = 0.1 # input\n",
    "\n",
    "pullbacks = Any[] # a vector where we store the pull backs\n",
    "r = x\n",
    "for k = 1:n\n",
    "    r,pâ‚– = pullback(sin, r) # new pullback\n",
    "    push!(pullbacks, pâ‚–)\n",
    "end\n",
    "r # value"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To deduce the derivative we need can either do forward- or back-back-propogation: loop through our pullbacks."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "forward_der = 1 # we always initialise with the trivial scaling\n",
    "for k = 1:n\n",
    "    forward_der = pullbacks[k](forward_der)[1]\n",
    "end\n",
    "\n",
    "reverse_der = 1 # we always initialise with the trivial scaling\n",
    "for k = n:-1:1\n",
    "    reverse_der = pullbacks[k](reverse_der)[1]\n",
    "end\n",
    "@test reverse_der â‰ˆ forward_der â‰ˆ ForwardDiff.derivative(x -> manysin(n, x), x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zygote constructs code that is equivalent to this loop automatically, and without the need for creating a \"tape\".\n",
    "That is, it doesn't actually create a vector `pullbacks` to record the operations at run-time, rather,\n",
    "it constructs a high-performance version of this back-propogation loop at compile time using something called source-to-source\n",
    "differentiation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem** Write a simple function for Taylor series of exponential.\n",
    "Write a function that implements back-propogation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Gradients and pullbacks\n",
    "\n",
    "Now we consider computing gradients, which is\n",
    "essential in ML.   Again we denote the Jacobian as\n",
    "$$\n",
    " J_f = \\begin{bmatrix} {âˆ‚ f_1 \\over âˆ‚x_1} & â‹¯ & {âˆ‚ f_1 \\over âˆ‚x_â„“} \\\\\n",
    "      â‹® & â‹± & â‹® \\\\\n",
    "      {âˆ‚ f_m \\over âˆ‚x_1} & â‹¯ & {âˆ‚ f_m \\over âˆ‚x_â„“}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that gradients are the transpose of Jacobians: $âˆ‡h = J_h^âŠ¤$.\n",
    "For a scalar-valued function $f : â„^n â†’ â„$ the pullback represents the linear map $â„ â†’ â„^n$\n",
    "corresponding to the gradient:\n",
    "$$\n",
    "ğ­ â†¦ J_f(ğ±)^âŠ¤ğ­ = âˆ‡f(ğ±) ğ­\n",
    "$$\n",
    "Here we see an example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f = (ğ±) -> ((x,y) = ğ±;  exp(x*cos(y)))\n",
    "ğ± = [0.1,0.2]\n",
    "f_v, f_pb = Zygote.pullback(f, ğ±)\n",
    "@test f_pb(1)[1] â‰ˆ [exp(x*cos(y))*cos(y), -exp(x*cos(y))*x*sin(y)]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a function $f : â„^n â†’ â„^m$ the the pullback represents the map $â„^m â†’ â„^n$ given by\n",
    "$$\n",
    "ğ­ â†¦ J_f(ğ±)^âŠ¤ğ­\n",
    "$$\n",
    "Here is an example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f = function(ğ±)\n",
    "    x,y,z = ğ±\n",
    "    [exp(x*y*z),cos(x*y+z)]\n",
    "end\n",
    "\n",
    "\n",
    "ğ± = [0.1,0.2,0.3]\n",
    "f_v, f_pb =  pullback(f, ğ±)\n",
    "\n",
    "J_f = function(ğ±)\n",
    "    x,y,z = ğ±\n",
    "    [y*z*exp(x*y*z) x*z*exp(x*y*z) x*y*exp(x*y*z);\n",
    "     -y*sin(x*y+z) -x*sin(x*y+z) -sin(x*y+z)]\n",
    "end\n",
    "\n",
    "ğ² = [1,2]\n",
    "@test J_f(ğ±)'*ğ² â‰ˆ f_pb(ğ²)[1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider a composition $f : â„^n â†’ â„^m$, $g : â„^m â†’ â„^â„“$ and $h : â„^â„“ â†’ â„$, that is,\n",
    "we want to compute the gradient of $h âˆ˜ g âˆ˜ f : â„^n â†’ â„$. The Chain rule tells us that\n",
    "$$\n",
    " J_{h âˆ˜ g âˆ˜ f}(ğ±) = J_h(g(f(ğ±)) J_g(f(ğ±)) J_f(ğ±)\n",
    "$$\n",
    "Put another way, the gradiant of $h âˆ˜ g âˆ˜ f$\n",
    "is given by the transposes of Jacobians:\n",
    "$$\n",
    "   âˆ‡[{h âˆ˜ g âˆ˜ f}](ğ±) = J_f(ğ±)^âŠ¤ J_g(f(ğ±))^âŠ¤  âˆ‡h(g(f(ğ±))\n",
    "$$\n",
    "Thus we have three pullbacks $p_1 : â„^m â†’ â„^n$, $p_2 : â„^â„“ â†’ â„^m$ and $p_3 : â„ â†’ â„^â„“$ given by\n",
    "\\begin{align*}\n",
    " p_1(ğ­) &= J_f(ğ±)^âŠ¤ ğ­  \\\\\n",
    " p_2(ğ­) &= J_g(f(x))^âŠ¤ ğ­  \\\\\n",
    " p_3(ğ­) &= âˆ‡h(g(f(ğ±)) ğ­\n",
    "\\end{align*}\n",
    "The gradient is given by _back-propagation_:\n",
    "$$\n",
    " p_1(p_2(p_3(1))) = J_f(ğ±)^âŠ¤ J_g(f(ğ±))^âŠ¤  âˆ‡h(g(f(ğ±)).\n",
    "$$\n",
    "Here the \"right\" order to do the multiplications is clear: matrix-matrix multiplications are expensive\n",
    "so its best to do it reverse order so that we only ever have matrix-vector multiplications.\n",
    "Also, the pullback doesn't give us enough information to implement forward-propagation.\n",
    "(This can be done using `Zygote.pushforward` which implements the transpose map $ğ­ â†¦ J_f(ğ±) ğ­$\n",
    "and is used in Zygote.jl for"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "z,pb = rrule(sum, [1, 2])\n",
    "\n",
    "Zygote.jacobian(x -> broadcast(sin, x), [1,2])[1]\n",
    "rrule(broadcast, sin, [1,2])\n",
    "\n",
    "unthunk(pb(1)[2])\n",
    "\n",
    "@ent Zygote.gradient(sum, [1,2])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Optimisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A key place where reverse-mode automatic differentiation is large scale optimisation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an extremely simple example we will look at the classic optimisation problem\n",
    "representing $A ğ± = ğ›$: find $ğ±$ that minimises\n",
    "$$\n",
    "f_{A,ğ›}(ğ±) = ğ±^âŠ¤ A ğ± - 2ğ±^âŠ¤ ğ›\n",
    "$$\n",
    "where $A$ is symmetric positive definite.\n",
    "Of course we can use tried-and-true techniques like the QR factorisation but here we want\n",
    "to emphasise we can also solve this with simple optimsation algorithms like gradient desecent\n",
    "which do not know the structure of the problem. We consider a matrix where we know gradient descent\n",
    "will converge fast:\n",
    "$$\n",
    "A = {n^2} \\begin{bmatrix} 1 & 1/4 \\\\ 1/4 & 1 & â‹± \\\\ &  â‹± & â‹± & 1/n^2 \\\\ && 1/n^2 & 1 \\end{bmatrix}\n",
    "$$\n",
    "In other words our functional has the form:\n",
    "$$\n",
    "f_{A,ğ›}(ğ±) = âˆ‘_{k=1}^n x_k^2 - âˆ‘_{k=2}^n x_{k-1} x_k/k^2 - âˆ‘_{k=1}^n x_k b_k\n",
    "$$\n",
    "For simplicity we will take $ğ›$ to be the vector with all ones.\n",
    "We need to write this in a vectorised way to ensure Zygote is sufficiently fast. We can efficiently\n",
    "compute gradients even\n",
    "with a million degrees of freedom, way beyond what could ever be done with forward-mode automatic differentiation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 1_000_000\n",
    "f = x -> (x'x + 2x[1:end-1]'*(x[2:end] ./ (2:n).^2)) - 2sum(x)\n",
    "\n",
    "x = randn(n) # initial guess\n",
    "Zygote.gradient(f, x) # compile\n",
    "@time Zygote.gradient(f, x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our algorithm is a quick-and-dirty gradient descent:\n",
    "$$\n",
    "x_{k+1} = x_k - Î³_k âˆ‡f(x_k)\n",
    "$$\n",
    "where $Î³_k$ is the learning rate. To choose $Î³_k$ we just halve\n",
    "the learning rate until we see decrease."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for k = 1:30\n",
    "    Î³ = 1\n",
    "    y = x - Î³*Zygote.gradient(f, x)[1]\n",
    "    while f(x) < f(y)\n",
    "        Î³ /= 2 # half the learning rate\n",
    "        y = x - Î³*Zygote.gradient(f, x)[1]\n",
    "    end\n",
    "    x = y\n",
    "    @show Î³,f(x)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compare this with the \"true\" solution:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "A = SymTridiagonal(ones(n), (2:n) .^ (-2))\n",
    "@test x â‰ˆ A\\ones(n)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Optimization, OptimizationOptimJL\n",
    "\n",
    "\n",
    "n = 1000\n",
    "f = (y,(a,b,h)) -> (sqrt(1 + ((y[1] - a)/h).^2) + sum(y[1:end-1] .* sqrt.(1 .+ ((y[2:end] - y[1:end-1])/h) .^2)) + sqrt(1 + ((b - y[end])/h).^2))*h\n",
    "y0 = ones(n-2) # drop boundary conditions\n",
    "prob = OptimizationProblem(OptimizationFunction(f, Optimization.AutoZygote()), y0, (1,1,1/n))\n",
    "@time y = solve(prob, BFGS()); plot(y)\n",
    "\n",
    "@time y = solve(prob, GradientDescent()); plot(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will do an example that will connect to neural networks."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 100\n",
    "x = range(-1, 1; length = n)\n",
    "y = exp.(x)\n",
    "\n",
    "function summation_model(x, (ğš, ğ›))\n",
    "    Y = relu.(ğš*x' .+ ğ›)\n",
    "    vec(sum(Y; dims=1)) # sums over the columns\n",
    "end\n",
    "\n",
    "convex_regression_loss((ğš, ğ›), (x,y)) = norm(summation_model(x, (ğš, ğ›)) - y)\n",
    "\n",
    "ğš,ğ› = randn(n),randn(n)\n",
    "plot(x, summation_model(x, (ğš, ğ›)))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can efficiently compute the gradient with respect to the parameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@time Zygote.gradient(convex_regression_loss, (ğš, ğ›), (x,y))[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss = convex_regression_loss((ğš,ğ›), (x,y))\n",
    "for k = 1:100\n",
    "    Î³ = 0.01\n",
    "    (a_n, b_n) = (ğš, ğ›) .- Î³.*Zygote.gradient(convex_regression_loss, (ğš, ğ›), (x,y))[1]\n",
    "    while loss < convex_regression_loss((a_n, b_n), (x,y))\n",
    "        Î³ /= 2 # half the learning rate\n",
    "        (a_n, b_n) = (ğš, ğ›) .- Î³.*Zygote.gradient(convex_regression_loss, (ğš, ğ›), (x,y))[1]\n",
    "    end\n",
    "    (ğš, ğ›) = (a_n, b_n)\n",
    "    global loss = convex_regression_loss((ğš,ğ›), (x,y))\n",
    "    @show Î³,loss\n",
    "end\n",
    "\n",
    "plot(x, y)\n",
    "plot!(x, summation_model(x, (ğš, ğ›)))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "prob = OptimizationProblem(OptimizationFunction(convex_regression_loss, Optimization.AutoZygote()), y0, (1,1,1/n))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
