{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SciML SANUM2024\n",
    "# Lab 6: Neural Differential Equations and DiffEqFlux.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this final lab we look at combining differential equations and\n",
    "neural networks, with the goal of \"learning\" dynamics based on training data.\n",
    "That is, consider an ODE of the form\n",
    "$$\n",
    "u' = f(u) + g(u)\n",
    "$$\n",
    "where we know $f$ (or if we don't know anything, $f = 0$) but don't know\n",
    "$g$. We can approximate $g$ by a neural network, and then we want to choose\n",
    "the parameters to fit data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we look at some simple examples, but the same techniques have been used\n",
    "in clinical trial accelleration for vaccine development by Moderna,\n",
    "climate change modelling and COVID prediction, see the [SciML Schowcase](https://sciml.ai/showcase/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Learning Outcomes**\n",
    "1. Combining neural networks and differential equations.\n",
    "2. Deducing dynamics by training a neural network.\n",
    "3. Using multiple optimisers to get good approximations.\n",
    "4. Neural ODEs as layers in a NN.\n",
    "5. Number classification via a NN with a Neural ODE layer."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Lux, DifferentialEquations, Optimization, OptimizationOptimisers, Plots, Zygote, SciMLSensitivity,\n",
    "            ComponentArrays, Random, LinearAlgebra, Test, Statistics"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1 Learning dynamics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin with a very simple ODE:\n",
    "$$\n",
    "u' = u - Œ± u^3\n",
    "$$\n",
    "where we know $f(u) = u$ but suppose we don't know $g(u) = -Œ± u^2$.\n",
    "First let's setup some training data with different initial conditions.\n",
    "We will do 10 trials which are sampled at 15 points for $t ‚àà [0,5]$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now try to deduce the term $-Œ±u^3$ by training a simple NN\n",
    "by minimising the error when comparing the model to the provided data.\n",
    "Because Optimzation.jl (currently) requires that parameters behave like\n",
    "arrays, rather than passing in the NN as a parameter we make it\n",
    "a global constant. We begin with simple 2-layer piecewise affine NN:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our model is\n",
    "$$\n",
    "  u' = u + g(u)\n",
    "$$\n",
    "where we represent $g$ by a NN with given parameters. Here is the rhs for this simple model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then compute the loss by solving the ODE with a given set of parameters\n",
    "for each of the runs in our samples and summing over the 2-norms of the error\n",
    "between our prediction and the data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are now ready to optimise. This will take some time so to avoid boredom\n",
    "and to understand how well the optimisation is working we will plot the\n",
    "model prediction of $g$ as we run the optimiser. To do this we provide\n",
    "a simple callback. This probably slows down the optimisation but is useful\n",
    "for us to see, and probably useful in practice to tell when the optimisation is\n",
    "stuck:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now setup the optimisation and run it 200 times:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We didn't do very well. Let's try changing the optimiser, passing in the previous solution\n",
    "as the initial guess:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This did much better and meets the ballpark norm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 1** Replace the neural network with a multilayer network and smooth activation\n",
    "function. Can you get better results than the simple RELU network?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: Construct a multilayer NN with smooth activation and see if it performs better"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 2** Use the predator-prey model\n",
    "$$\n",
    "\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} =  \\begin{bmatrix}Œ±x - Œ≤xy \\\\  Œ¥xy - Œ≥y\\end{bmatrix}\n",
    "$$\n",
    "on $T ‚àà [0,5]$ with $Œ± , Œ≤,Œ¥,Œ≥ = 1,2,3,4$ with initial condition $[1,2]$\n",
    "to generate training data of samples at 21 evenly spaced points (only do a single run).\n",
    "Suppose we do not know the whole interaction but can model\n",
    "$$\n",
    " \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} =  \\begin{bmatrix}Œ±x \\\\ - Œ≥y\\end{bmatrix} + g(x,y)\n",
    "$$\n",
    "where $g :‚Ñù^2 ‚Üí ‚Ñù^2$ is modeled by a Neural Network. Deduce $g$ by optimization of a loss when\n",
    "compared to the training data.\n",
    "Hint: This [SciML example](https://docs.sciml.ai/Overview/stable/showcase/missing_physics/)\n",
    "solves this problem and might help guide you."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: Learn the dynamics in a predator-prey model."
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 Neural ODEs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A neural ODE is essentially the same as an ODE where the right-hand side\n",
    "is a neural network, i.e., we represent the solution to the ODE\n",
    "$$\n",
    "ùêÆ' = f(ùêÆ)\n",
    "$$\n",
    "but where $f$ is given by a Neural Network.\n",
    "This is a more specialised version of\n",
    "the above but without the case where we incorporating parts of the model that are known.\n",
    "The idea is that they can be used as layers in more complicated Neural Networks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can create a Neural ODE using the `NeuralODE` type in DiffEqFlux.jl\n",
    "(despite the name, DiffEqFlux.jl now uses Lux.jl instead of the older similar package Flux.jl):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compare this to the same solve as an ODE problem:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#\n",
    "\n",
    "#¬†Unlike the solve as above we use Neural ODEs in Chains."
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's a simple artificial example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To understand why this is useful relates to how one designs neural networks to\n",
    "match the problem, something we're not going to dig into. But in the next section\n",
    "we show how it can be used to solve a fun real world problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 Image classification via Neural ODEs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our final example, we're going to look at the go-to problem of classifying numbers\n",
    "given an image, specified as pixels, as in the MNIST database.\n",
    "We are going to walk through one of the [standard examples](https://docs.sciml.ai/DiffEqFlux/stable/examples/mnist_neural_ode/)\n",
    "in DiffEqFlux.jl but simplified (the original example supports GPUs)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First let's load the database:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a database which contains an image (in the \"features\" key)\n",
    "and what number that image represents (in the \"targets\" key)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a database of 60k images.\n",
    "We can see an example here:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can plot the image by converting the elements to `Gray` in which case\n",
    "Images.jl automatically plots. We transpose the pixels since the default\n",
    "has $x$ and $y$ axes swapped:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can recognise this is a 6, and the database tells us this information:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to create a NN with a Neural ODE layer to approximate the map\n",
    "from a 28 √ó 28 image (represented by `Float64`) to a number.\n",
    "But having the output be a number isn't quite enough since we can only be approximately\n",
    "accurate. Therefore we want to map from an image to a 10-vector where the\n",
    "entry with the largest value is the number (+1), and other entries somehow give us extra information\n",
    "about the chance that its that number. Eg. we want to use a\n",
    "so-called \"one hot\" encoding of the number. We can do so with the following function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus we want to construct a map from an image to a 10-vector and we represent\n",
    "this map by a NN, one with a NeuralODE layer. Without digging into the motivation\n",
    "we will follow the example."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we want to \"downsample\" an image from $‚Ñù^{28 √ó 28}$\n",
    "to $‚Ñù^{20}$, but we also want to work with $N$ images at the same time\n",
    "for efficiency reasons (\"batching\").\n",
    "The first step is to flatten matrices to vectors using a `FlattenLayer`\n",
    "which is map from $‚Ñù^{28 √ó 28 √ó N} ‚Üí ‚Ñù^{28^2 √ó N}$ by just vectorising the\n",
    "images by columns:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now combine it with a Neural network to get a map ${\\rm down} : ‚Ñù^{28 √ó 28 √ó N} ‚Üí ‚Ñù^{20 √ó N}$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next layer is going to be a map from initial  conditions to final values\n",
    "$‚Ñù^{20 √ó N} ‚Üí ‚Ñù^{20 √ó N}$ where we feed\n",
    "each of the outputs into a Neural ODE whose RHS is given by another NN:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since our output needs to be a 10-vector we finally pass it through one last layer\n",
    "that down samples $‚Ñù^{20 √ó N} ‚Üí ‚Ñù^{10 √ó N}$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can put everything together. A nice feature of Lux.jl is we can give names to our\n",
    "layers (more descriptive than `layer_1`) as follows (here the `convert` layer maps a solution\n",
    "to an ODE to its final value which we need to wrap in order for it to work with Lux):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our NN is pretty fast! But the parameters to actually give us the right out:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "So now we want to setup a loss function to choose the parameters.\n",
    "This will be based on matching data.\n",
    "For efficiency we need to group our data into batches:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to choose the parameters in our model to map `train[b][1]` to `train[b][2]`\n",
    "for every batch `b`.\n",
    "But we simply want to measure the largest components are in the same spot, not necessarily\n",
    "that the $k$-th entry is close to 1 and all other entries are close to 0.\n",
    "Statistics/information theory tells us that  following gives us a good loss function\n",
    "for imposing this (which is beyond my ken):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see how our random parameters do:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now optimise the paramters to fit the data. Let's setup the optimisation problem:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll make a callback to measure the progress:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally we can train the NN-ODE and monitor the loss and weights."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Did it work? Let's try an image in the database:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Certainly not a \"onehot\" vector! But we can recover the predicted value:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It was correct! But what about the images not in our training set?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This may or may not be correct depending on the training:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "But we can find the percentage it gets right:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    ">90% accuracy is pretty good!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4 Final remarks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have only dipped our toes into the very basics of SciML.\n",
    "The [SciML website](https://sciml.ai) is a good place for more serious examples."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is also a closely related area of Physics Informed Neural Networks (PINNs)\n",
    "where one tries to replace ODE solvers with NNs by training them on input-output pairs.\n",
    "Based on what we have seen, I am highly sceptical this is useful for low dimensional problems\n",
    "where we have very good numerical methods that not only achieve \"eyeball norm\" but sometimes\n",
    "much more accuracy (as much as even 16 digits!).\n",
    "But for high-dimensional problems the number classification  problem gives some indication\n",
    "that these methods can play a role for producing \"ballpark norm\" approximations.\n",
    "Unfortunately its much harder to do a sanity check for problems where we can't necessarily\n",
    "look at the output and say \"yeah, that's an 8 and the algorithm thinks its an 8\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I think a bigger impact of this technology is the robustness of automatic differentiation\n",
    "which can play a big role in classical applied mathematics without necessarily using neural\n",
    "networks. I think the future of numerical algorithms will very much consider the ability to\n",
    "perform automatic differentiation efficiently. The SciML developers, in particular Chris Rackackas,\n",
    "have also realised that the role of stiff versus non-stiff and stability of forward versus reverse\n",
    "automatic differentiation are intrinsically linked, so there is a big need for classical numerical analysis\n",
    "even in this brave new world."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
