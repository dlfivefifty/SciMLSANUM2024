{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciML SANUM2024\n",
    "# Lab 3: Reverse-mode automatic differentiation and Zygote.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of unknowns becomes large forward-mode automatic differentiation as\n",
    "implemented in ForwardDiff.jl becomes prohibitively expensive for computing gradients and instead we need to\n",
    "use reverse-mode automatic differentiation: this is best thought of as implementing the chain-rule\n",
    "in an automatic fashion, with a specific choice of multiplying the underlying Jacobians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing gradients is important for solving optimisation problems, which is what ultimately what training a neural network\n",
    "is. Therefore we also look at solving some\n",
    "simple optimissation problems, using Optimsation.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Outcomes**\n",
    "\n",
    "1. Computing gradients and derivatives with Zygote.jl\n",
    "2. Basics of reverse-mode automatic differentiation and pullbacks.\n",
    "3. Forward-mode automatic differentiation via pushforwards.\n",
    "4. Using automatic differentiation for implementing gradient descent.\n",
    "5. Solving optimisation with gradient descent and via Optimsation.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Using Zygote.jl for differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a simple demonstration of Zygote.jl, which can be thought of as a replacement for ForwardDiff.jl that\n",
    "uses reverse-mode differentiation under the hood. We can differentiate scalar functions, but unlike ForwardDiff.jl it\n",
    "overloads the `'` syntax to mean differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Zygote, Test\n",
    "\n",
    "f = x -> exp(x^2)\n",
    "f(0.1), f'(0.1) # \"adjoint\" is overloaded for functions to mean derivative\n",
    "\n",
    "@test exp'(1) == exp(1)\n",
    "@test cos'(1) == -sin(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real power of Zygote.jl is computing gradients (or more generally, Jacobians\n",
    "of $f : ℝ^m → ℝ^n$ where $n ≪ m$). We can compute a gradient of the function we considered before as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.172910 seconds (435.58 k allocations: 782.390 MiB, 19.09% gc time, 41.41% compilation time)\n",
      "  0.188706 seconds (650.95 k allocations: 45.993 MiB, 1.93% gc time, 66.66% compilation time)\n"
     ]
    }
   ],
   "source": [
    "f = function(x)\n",
    "    ret = zero(eltype(x))\n",
    "    for k = 1:length(x)-1\n",
    "        ret += x[k]*x[k+1]\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "using ForwardDiff\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike ForwardDiff.jl, the gradient returns a tuple since multiple arguments are supported in addition\n",
    "to vector inputs, eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.3203170336021262, 1.3471707570217752], -0.02676415127641921)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gradient((x,y) -> exp(x[1]*cos(y) + x[2]), [0.1,0.2], 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now differentiating this function is not particularly faster than ForwardDiff.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10_000\n",
    "x = randn(n)\n",
    "\n",
    "@time gradient(f, x)[1] # returns a Tuple with a single entry\n",
    "@time ForwardDiff.gradient(f, x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also uses more memory the larger the computation. Take for example\n",
    "the Taylor series for the exponential from Lab 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000005 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "#45 (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function exp_t(z, n)\n",
    "    ret = one(z)\n",
    "    s = one(z)\n",
    "    for k = 1:n\n",
    "        s = s/k * z\n",
    "        ret = ret + s\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "@time exp_t(1.0, 1000)\n",
    "exp_t_1000 = x -> exp_t(x, 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000147 seconds (1 allocation: 16 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.389056098930649"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time ForwardDiff.derivative(exp_t_1000, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.010836 seconds (250.07 k allocations: 8.185 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time exp_t_1000'(1.0) # Zygotes memory grows with the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more terms we take the more memory is used, despite the function itself\n",
    "using no memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another catch is Zygote.jl doesn't support functions that mutate arrays. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Mutating arrays is not supported -- called setindex!(Vector{Float64}, ...)\nThis error occurs when you ask Zygote to differentiate operations that change\nthe elements of arrays in place (e.g. setting values with x .= ...)\n\nPossible fixes:\n- avoid mutating operations (preferred)\n- or read the documentation and solutions for this error\n  https://fluxml.ai/Zygote.jl/latest/limitations\n",
     "output_type": "error",
     "traceback": [
      "Mutating arrays is not supported -- called setindex!(Vector{Float64}, ...)\nThis error occurs when you ask Zygote to differentiate operations that change\nthe elements of arrays in place (e.g. setting values with x .= ...)\n\nPossible fixes:\n- avoid mutating operations (preferred)\n- or read the documentation and solutions for this error\n  https://fluxml.ai/Zygote.jl/latest/limitations\n",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base ./error.jl:35",
      " [2] _throw_mutation_error(f::Function, args::Vector{Float64})",
      "   @ Zygote ~/.julia/packages/Zygote/jxHJc/src/lib/array.jl:70",
      " [3] (::Zygote.var\"#539#540\"{Vector{Float64}})(::Nothing)",
      "   @ Zygote ~/.julia/packages/Zygote/jxHJc/src/lib/array.jl:82",
      " [4] (::Zygote.var\"#2623#back#541\"{Zygote.var\"#539#540\"{Vector{Float64}}})(Δ::Nothing)",
      "   @ Zygote ~/.julia/packages/ZygoteRules/M4xmc/src/adjoint.jl:72",
      " [5] foo",
      "   @ ./In[43]:5 [inlined]",
      " [6] (::Zygote.Pullback{Tuple{typeof(foo), Vector{Float64}}, Any})(Δ::Float64)",
      "   @ Zygote ~/.julia/packages/Zygote/jxHJc/src/compiler/interface2.jl:0",
      " [7] (::Zygote.var\"#75#76\"{Zygote.Pullback{Tuple{typeof(foo), Vector{Float64}}, Any}})(Δ::Float64)",
      "   @ Zygote ~/.julia/packages/Zygote/jxHJc/src/compiler/interface.jl:91",
      " [8] gradient(::Function, ::Vector{Float64}, ::Vararg{Any})",
      "   @ Zygote ~/.julia/packages/Zygote/jxHJc/src/compiler/interface.jl:148",
      " [9] top-level scope",
      "   @ In[43]:12"
     ]
    }
   ],
   "source": [
    "function foo(x)\n",
    "    n = length(x)\n",
    "    ret = zeros(eltype(x), n)\n",
    "    for k = 1:n\n",
    "        ret[k] = x[k] + k\n",
    "    end\n",
    "    sum(ret)\n",
    "end\n",
    "\n",
    "x = randn(5)\n",
    "ForwardDiff.gradient(foo,x)\n",
    "gradient(foo,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is unlike `ForwardDiff.gradient` which works fine for differentiating `f!`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why do we use reverse-mode automatic differentiation when it has so many weaknesses\n",
    "compared to forward-mode?\n",
    "Because if we write code in just the right way it becomes extremely fast.\n",
    "For example, if we rewrite `f` in a vectorised form we see a huge improvement over\n",
    "ForwardDiff.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = function(x)\n",
    "    ret = zero(eltype(x))\n",
    "    for k = 1:length(x)-1\n",
    "        ret += x[k]*x[k+1]\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "x = 1:5\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×9 adjoint(::Matrix{Float64}) with eltype Float64:\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian(x -> x[1:end-1], randn(10))[1]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " 0.107488  0.0       -0.0\n",
       " 0.0       0.197026  -0.0\n",
       " 0.0       0.0       -0.110458"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian(x -> x[2:end], randn(10))[1]'\n",
    "\n",
    "jacobian(broadcast, *, randn(3), randn(3))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " 0.211531  0.0      -0.0\n",
       " 0.0       1.00186  -0.0\n",
       " 0.0       0.0      -0.424847"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian(broadcast, *, randn(3), randn(3))[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = x -> sum(x[1:end-1] .* x[2:end]) # a \"vectorised\" version of above\n",
    "f2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.002140 seconds (26.06 k allocations: 8.699 MiB)\n",
      "  0.000041 seconds (25 allocations: 65.906 KiB)\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "x = randn(n)\n",
    "@time gradient(f, x); # very slow O(n^2) complexity\n",
    "@time gradient(f2, x); # very fast O(n) complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.012759 seconds (33 allocations: 61.037 MiB)\n"
     ]
    }
   ],
   "source": [
    "n = 1_000_000\n",
    "x = randn(n)\n",
    "@time gradient(f2, x); # very fast O(n) complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Zygote.jl is much more brittle, sometimes fails outright, requires\n",
    "writing functions in a specific way, uses a lot more memory to record complicated operations, but when it works\n",
    "well it is _extremely_ fast. Thus when we get to neural networks it is paramount that\n",
    "we design our representations of neural networks in a way that is ameniable to reverse-mode\n",
    "automatic differentiation, as implemented in Zygote.jl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Pullbacks and back-propagation for scalar functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now peek a little under-the-hood to get some intuition on how Zygote.jl is computing\n",
    "derivatives, and to understand why its so much faster than ForwardDiff.jl in certain situations. Underlying automatic\n",
    "differentiation in Zygote.jl are so-called \"pullback\"s. In the scalar\n",
    "case these are very close to the notion of a derivative. However, rather than\n",
    "the derivative being a single constant, it's a linear map representing the derivative:\n",
    "eg, if the derivative of $f(x)$ is denoted $f'(x)$ then the pullback is a linear map\n",
    "$$\n",
    "t ↦ f'(x)t.\n",
    "$$\n",
    "We can compute pullbacks using the `pullback` routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09983341664682815, Zygote.var\"#75#76\"{Zygote.ZBack{ChainRules.var\"#sin_pullback#1289\"{Float64}}}(Zygote.ZBack{ChainRules.var\"#sin_pullback#1289\"{Float64}}(ChainRules.var\"#sin_pullback#1289\"{Float64}(0.9950041652780258))))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pullback # provided by Zygote\n",
    "s, p_sin = pullback(sin, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`p_sin` contains the map $t ↦ \\cos(0.1) t$. Since pullbacks support multiple arguments\n",
    "it actually returns a tuple with a single entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 1\n",
    "@test p_sin(t)[1] == cos(0.1)*t\n",
    "t = 2\n",
    "@test p_sin(t)[1] == cos(0.1)*t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus to get out the value we use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason its a map instead of just a scalar becomes important for the vector-valued case\n",
    "where Jacobians can often be applied to vectors much faster than constructing the Jacobian matrix and\n",
    "performing a matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pullbacks can be used for determining more complicated derivatives. Consider a composition of three functions\n",
    "$h ∘ g ∘ f$ where from the Chain Rule we know:\n",
    "$$\n",
    "{{\\rm d} \\over {\\rm d} x}[h(g(f(x))] = h'(g(f(x)) g'(f(x)) f'(x)\n",
    "$$\n",
    "Essentially we have three pullbacks: the first is the pullback of $f$ evaluated\n",
    "at $x$, the second corresponding to $g$ evaluated at $f(x)$, and the third\n",
    "corresponding to $h$ evaluated at $g(f(x))$, that is:\n",
    "$$\n",
    "\\begin{align*}\n",
    " p_1(t) &= f'(x) t  \\\\\n",
    " p_2(t) &= g'(f(x)) t  \\\\\n",
    " p_3(t) &= h'(g(f(x))t\n",
    "\\end{align*}\n",
    "$$\n",
    "Thus the derivative is given by either the _forward_ or _reverse_ composition of these functions:\n",
    "$$\n",
    " p_3(p_2(p_1(1))) = p_1(p_2(p_3(1))) = h'(g(f(x))g'(f(x))f'(x).\n",
    "$$\n",
    "The first version is called _forward-propagation_ and the second called _back-propagation_.\n",
    "Forward-propagation is a version of forward-mode automatic differentiation and is essentially equivalent to using dual numbers.\n",
    "We will see later in the vector case that forward- and back-propagation are not the same,\n",
    "and that back-propagation is much more efficient provided the output is scalar (or small dimensional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see pullbacks in action for computing the derivative of $\\cos\\sqrt{{\\rm e}^x}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = exp\n",
    "g = sqrt\n",
    "h = cos\n",
    "(x -> h(g(f(x))))'(0.1)\n",
    "\n",
    "x = 0.1\n",
    "y,p₁ = pullback(f, x) # y == f(x), p₁ = t -> f'(x)*t\n",
    "z,p₂ = pullback(g, y) # z == g(f(x)), p₂ = t -> g'(f(x))*t\n",
    "w,p₃ = pullback(h, z) # w == h(g(f(x))), p₃ = t -> h'(g(f(x)))*t\n",
    "\n",
    "@test w == h(g(f(x)))\n",
    "@test p₁(1)[1] == f'(x)\n",
    "@test p₂(p₁(1)[1])[1] == g'(f(x))*f'(x)\n",
    "@test p₃(p₂(p₁(1)[1])[1])[1] ≈ h'(g(f(x)))g'(f(x))*f'(x)\n",
    "\n",
    "@test p₁(p₂(p₃(1)[1])[1])[1] ≈ h'(g(f(x)))g'(f(x))*f'(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f(x...) is a splat the same as f(x[1], x[2], …, x[end])\n",
    "@test p₁(p₂(p₃(1)...)...)[1] ≈ h'(g(f(x)))g'(f(x))*f'(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how this can lead to an approach for automatic differentiation.\n",
    "For example, consider the following function composing `sin` over and over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note composition is all we need: x+y is +(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0991753231269109"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function manysin(n, x) # sin(sin(…sin(x))) composed n times\n",
    "    r = x\n",
    "    for k = 1:n\n",
    "        r = sin(r)\n",
    "    end\n",
    "    r\n",
    "end\n",
    "\n",
    "# Why does Zygote use more memory even though this function uses no memory?\n",
    "n = 5\n",
    "x =0.1\n",
    "manysin(n, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would need `n` pullbacks as each time `sin` is called at a different value.\n",
    "But the number of such pullbacks grows only linearly so this is acceptable. So thus\n",
    "at a high-level we can think of Zygote as running through and computing all the pullbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pullbacks = Any[] # Any[] means an empty vector whose entries are anything\n",
    "\n",
    "r = x\n",
    "\n",
    "for k = 1:n\n",
    "    r,pₖ = pullback(sin, r) # r changes each time so pullback is cos(r)*t\n",
    "    push!(pullbacks, pₖ) # add new pullback to end of the vector\n",
    "end\n",
    "@test pullbacks[1](2)[1] ≈ 2*cos(0.1)\n",
    "@test pullbacks[2](2)[1] ≈ 2*cos(sin(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deduce the derivative we need can either do forward- or back-propogation by looping through our pullbacks\n",
    "either in forward- or in reverse-order. Here we implement back-propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9754309720849006"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_d = 1\n",
    "for k = n:-1:1\n",
    "   r_d = pullbacks[k](r_d)[1]\n",
    "end\n",
    "r_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_d === (x->manysin(n,x))'(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zygote constructs code that is equivalent to this loop automatically,\n",
    "constructing a high-performance version of this back-propogation loop at compile time using something called source-to-source\n",
    "differentiation. But there's no getting around the fact that it needs to record the pullbacks so it does use more memory the larger\n",
    "the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#93 (generic function with 1 method)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x -> exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Any}:\n",
       " #97 (generic function with 1 method)\n",
       " #97 (generic function with 1 method)\n",
       " #97 (generic function with 1 method)\n",
       " #97 (generic function with 1 method)\n",
       " #97 (generic function with 1 method)\n",
       " #97 (generic function with 1 method)\n",
       " #97 (generic function with 1 method)\n",
       " #97 (generic function with 1 method)\n",
       " #97 (generic function with 1 method)\n",
       " #97 (generic function with 1 method)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manyanons = Any[]\n",
    "for k = 1:10\n",
    "    push!(manyanons, x -> exp(k*x))\n",
    "end\n",
    "manyanons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1** Compute the derivative of `manysin` using forward-propagation, by looping through the pull-backs\n",
    "in the forward direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: loop through pullbacks in order to compute the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pullbacks with multiple arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things become more complicated when we have a function with multiple arguments, even in the\n",
    "scalar case. Consider now the function $f(g(x), h(x))$. The chain rule tells us that\n",
    "$$\n",
    "{{\\rm d} \\over {\\rm d} x}[f(g(x), h(x))] = f_x(g(x), h(x)) g'(x) + f_y(g(x), h(x)) h'(x)\n",
    "$$\n",
    "Now we have three pullbacks:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_1(t) &= g'(x) t\\\\\n",
    "p_2(t) &= h'(x) t\\\\\n",
    "p_3(t) &= [f_x(g(x), h(x))t, f_y(g(x), h(x))t]\n",
    "\\end{align*}\n",
    "$$\n",
    "In this case the derivative can be recovered via back-propagation via:\n",
    "$$\n",
    "p_1(p_3(1)[1]) + p_2(p_3(1)[2]).\n",
    "$$\n",
    "Here we see a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7171823264540017"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = (x,y) -> cos(x*exp(y))\n",
    "g = sqrt\n",
    "h = sin\n",
    "F = x -> f(g(x), h(x))\n",
    "x = 0.1\n",
    "F'(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gx,p₁ = pullback(g,x)\n",
    "hx,p₂ = pullback(h,x)\n",
    "z,p₃ = pullback(f, gx, hx)\n",
    "@test z == F(x)\n",
    "@test p₁(p₃(1)[1])[1] + p₂(p₃(1)[2])[1] == F'(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing more complicated calculations or indeed algorithms becomes\n",
    "quite complicated if there are interdependencecies, eg, $f(g(r(x)), h(r(x)))$.\n",
    "This explains why our first version of a function summing over products of its arguments\n",
    "was so slow.\n",
    "Fortunately, there is an alternative: we can focus on composing vector functions.\n",
    "Eg, such a function can be thought of as composition:\n",
    "$$\n",
    "f ∘ 𝐠 ∘ r\n",
    "$$\n",
    "where $𝐠(x) = [g(x),h(x)]$. This is a special case of what we discuss in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Gradients and pullbacks\n",
    "\n",
    "Now we consider computing gradients of functions that are compositions\n",
    "of vector functions, which neural networks fall into.\n",
    "Again, we denote the Jacobian as\n",
    "$$\n",
    " J_f = \\begin{bmatrix} {∂ f_1 \\over ∂x_1} & ⋯ & {∂ f_1 \\over ∂x_ℓ} \\\\\n",
    "      ⋮ & ⋱ & ⋮ \\\\\n",
    "      {∂ f_m \\over ∂x_1} & ⋯ & {∂ f_m \\over ∂x_ℓ}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that gradients are the transpose of Jacobians: $∇h = J_h^⊤$.\n",
    "For a scalar-valued function $f : ℝ^n → ℝ$ the pullback represents the linear map\n",
    "$p_{f,𝐱} : ℝ → ℝ^n$ corresponding to scaling the gradient:\n",
    "$$\n",
    "p_{f,𝐱}(t) = J_f(𝐱)^⊤t = ∇f(𝐱) t\n",
    "$$\n",
    "Here we see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = function(𝐱)\n",
    "    x,y = 𝐱\n",
    "    exp(x*cos(y))\n",
    "end\n",
    "𝐱 = [0.1,0.2]\n",
    "f_v, p_f = pullback(f, 𝐱)\n",
    "\n",
    "t = 2\n",
    "@test p_f(t)[1] == gradient(f, 𝐱)[1]*t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a function $f : ℝ^n → ℝ^m$ the the pullback represents the linear map $p_{f,𝐱} : ℝ^m → ℝ^n$ given by\n",
    "$$\n",
    "p_{f,𝐱}(t) = J_f(𝐱)^⊤𝐭\n",
    "$$\n",
    "Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 20-codeunit String at index [1:21]",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "BoundsError: attempt to access 20-codeunit String at index [1:21]",
      "",
      "Stacktrace:",
      " [1] checkbounds",
      "   @ ./strings/basic.jl:216 [inlined]",
      " [2] getindex(s::String, r::UnitRange{Int64})",
      "   @ Base ./strings/string.jl:468",
      " [3] complete_request(socket::ZMQ.Socket, msg::IJulia.Msg)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/handlers.jl:115",
      " [4] #invokelatest#2",
      "   @ ./essentials.jl:892 [inlined]",
      " [5] invokelatest",
      "   @ ./essentials.jl:889 [inlined]",
      " [6] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [7] (::IJulia.var\"#15#18\")()",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:38"
     ]
    },
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 20-codeunit String at index [1:21]",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "BoundsError: attempt to access 20-codeunit String at index [1:21]",
      "",
      "Stacktrace:",
      " [1] checkbounds",
      "   @ ./strings/basic.jl:216 [inlined]",
      " [2] getindex(s::String, r::UnitRange{Int64})",
      "   @ Base ./strings/string.jl:468",
      " [3] complete_request(socket::ZMQ.Socket, msg::IJulia.Msg)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/handlers.jl:115",
      " [4] #invokelatest#2",
      "   @ ./essentials.jl:892 [inlined]",
      " [5] invokelatest",
      "   @ ./essentials.jl:889 [inlined]",
      " [6] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [7] (::IJulia.var\"#15#18\")()",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:38"
     ]
    },
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 4-codeunit String at index [1:5]",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "BoundsError: attempt to access 4-codeunit String at index [1:5]",
      "",
      "Stacktrace:",
      " [1] checkbounds",
      "   @ ./strings/basic.jl:216 [inlined]",
      " [2] getindex(s::String, r::UnitRange{Int64})",
      "   @ Base ./strings/string.jl:468",
      " [3] complete_request(socket::ZMQ.Socket, msg::IJulia.Msg)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/handlers.jl:115",
      " [4] #invokelatest#2",
      "   @ ./essentials.jl:892 [inlined]",
      " [5] invokelatest",
      "   @ ./essentials.jl:889 [inlined]",
      " [6] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [7] (::IJulia.var\"#15#18\")()",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:38"
     ]
    }
   ],
   "source": [
    "f = function(𝐱) # map from R^3 to R^2\n",
    "    x,y,z = 𝐱\n",
    "    [exp(x*cos(y)*z), cos(x*y+z)]\n",
    "end\n",
    "𝐱 = [0.1,0.2,0.3]\n",
    "f_x, p_f = pullback(f,𝐱)\n",
    "\n",
    "t = [1,1]\n",
    "@test p_f(t)[1] == jacobian(f, 𝐱)[1]' * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a composition $f : ℝ^n → ℝ^m$, $g : ℝ^m → ℝ^ℓ$ and $h : ℝ^ℓ → ℝ$, that is,\n",
    "we want to compute the gradient of $h ∘ g ∘ f : ℝ^n → ℝ$. The Chain rule tells us that\n",
    "$$\n",
    " J_{h ∘ g ∘ f}(𝐱) = J_h(g(f(𝐱)) J_g(f(𝐱)) J_f(𝐱)\n",
    "$$\n",
    "Put another way, the gradiant of $h ∘ g ∘ f$\n",
    "is given by the transposes of Jacobians:\n",
    "$$\n",
    "   ∇[{h ∘ g ∘ f}](𝐱) = J_f(𝐱)^⊤ J_g(f(𝐱))^⊤  ∇h(g(f(𝐱))\n",
    "$$\n",
    "Thus we have three pullbacks $p_1 : ℝ^m → ℝ^n$, $p_2 : ℝ^ℓ → ℝ^m$ and $p_3 : ℝ → ℝ^ℓ$ given by\n",
    "\\begin{align*}\n",
    " p_1(𝐭) &= J_f(𝐱)^⊤ 𝐭  \\\\\n",
    " p_2(𝐭) &= J_g(f(x))^⊤ 𝐭  \\\\\n",
    " p_3(t) &= ∇h(g(f(𝐱)) t\n",
    "\\end{align*}\n",
    "The gradient is given by _back-propagation_:\n",
    "$$\n",
    " p_1(p_2(p_3(1))) = J_f(𝐱)^⊤ J_g(f(𝐱))^⊤  ∇h(g(f(𝐱)).\n",
    "$$\n",
    "Here the \"right\" order to do the multiplications is clear: matrix-matrix multiplications are expensive\n",
    "so its best to do it reverse order so that we only ever have matrix-vector multiplications.\n",
    "Also, the pullback doesn't give us enough information to implement forward-propagation:\n",
    "we don't have access to the Jacobian matrices, or their application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example consider computing the gradient of an iteration a simple map like:\n",
    "$$\n",
    "𝐟(x,y,z) = \\begin{bmatrix} \\cos(xy)+z\\\\ zy-\\exp(x)\\\\ x + y + z \\end{bmatrix}\n",
    "$$\n",
    "and summing over the result, eg. computing $[1,1,1]^⊤(\\underbrace{𝐟 ∘ ⋯ ∘ 𝐟}_{n\\hbox{ times}})(𝐱)$.\n",
    "We implement this with a general function `iteratef`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.783399764711769"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "𝐟 = function(𝐱)# map from R^3 to R^3\n",
    "    x,y,z = 𝐱\n",
    "    [cos(x*y)+z, z*y-sin(x), x+y+z]\n",
    "end\n",
    "\n",
    "𝐱 = [0.1,0.2,0.3]\n",
    "r,p_f = pullback(𝐟, 𝐱)\n",
    "t = [1,2,3]\n",
    "p_f(t)[1] == jacobian(𝐟, 𝐱)[1]'*t\n",
    "\n",
    "function iteratef(𝐱, f, n) # compose f many times and then sum\n",
    "    for k = 1:n\n",
    "        𝐱 = f(𝐱)\n",
    "    end\n",
    "    sum(𝐱)\n",
    "end\n",
    "\n",
    "iteratef(𝐱, 𝐟, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -6.5923400545604744\n",
       " 43.51048331198635\n",
       " 86.81831662174272"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(𝐱 -> iteratef(𝐱, 𝐟, 5), 𝐱)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-13.783399764711769, Zygote.var\"#75#76\"{Zygote.var\"#2989#back#768\"{Zygote.var\"#762#766\"{Vector{Float64}}}}(Zygote.var\"#2989#back#768\"{Zygote.var\"#762#766\"{Vector{Float64}}}(Zygote.var\"#762#766\"{Vector{Float64}}([1.7204985482335622, -12.007870102051191, -3.4960282108941403]))))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pullbacks = Any[] # Any[] means an empty vector whose entries are anything\n",
    "\n",
    "r = 𝐱\n",
    "\n",
    "for k = 1:n\n",
    "    r,pₖ = pullback(𝐟, r) # r changes each time so pullback is cos(r)*t\n",
    "    push!(pullbacks, pₖ) # add new pullback to end of the vector\n",
    "end\n",
    "\n",
    "ret,sumpullback = pullback(sum, r) # sum maps R^3 to R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea how this works behind the scenes we can again accumulate the pullbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recover the gradient by back-propogation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_g = 1\n",
    "\n",
    "r_g = Vector(sumpullback(r_g)[1])\n",
    "for k = n:-1:1\n",
    "    r_g = pullbacks[k](r_g)[1]\n",
    "end\n",
    "@test r_g == gradient(𝐱 -> iteratef(𝐱, 𝐟, 5), 𝐱)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This explains why reverse-mode is fast: because we only have \n",
    "# matrix-vector products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we match the gradient as computed with Zygote.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2** The function `pushforward` represent the map $𝐭 ↦ J_f(𝐱) 𝐭$.\n",
    "Compute the gradient of `iteratef` as above with forward-mode automatic differentiation by using `pushforward`.\n",
    "Do so without creating a vector of pushforwards.\n",
    "Hint: We need to run the pushforward iteration with the identity matrix as the initial value,\n",
    "but the result of  `pushforward` only works on vectors. So we need to apply it to each column of the matrix manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the gradient as above but using pushforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3** Consider a simple forward Euler method approximating the solution to the Pendulum equation with friction:\n",
    "$$\n",
    "u'' = τ u' - \\sin u\n",
    "$$\n",
    "which we can rewrite as a first order system:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   u' \\\\\n",
    "   v'\n",
    "   \\end{bmatrix} = \\begin{bmatrix} v \\\\ -τ*v - \\sin u \\end{bmatrix}\n",
    "$$\n",
    "That is, we want to implement the iteration\n",
    "$$\n",
    "𝐮_{k+1} = 𝐮_k + h*\\begin{bmatrix} 𝐮_k[2] \\\\ -τ 𝐮_k[2] - \\sin 𝐮_k[1] \\end{bmatrix}\n",
    "$$\n",
    "with a specified initial condition $𝐮_0$. For $N = 100$, $h = 0.1$ and $𝐮_0 = [0.1,0.2]$, differentiate\n",
    "the solution with-respect to $τ$ at $τ = 1$ by creating a vector of pullbacks and implementing back-propagation.\n",
    "Hint: Forward Euler is a variant of `iteratef` above so you can modify the subsequent pullback construction. Add $τ$ to the vector\n",
    "of values to capture the relevant dependencies and verify your result by comparing to `gradient`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key place where reverse-mode automatic differentiation is essential is large scale optimisation.\n",
    "As a  simple example we will look at the classic optimisation problem\n",
    "that solves $A 𝐱 = 𝐛$ where $A$ is symmetric positive definite: find $𝐱$ that minimises\n",
    "$$\n",
    "f_{A,𝐛}(𝐱) = 𝐱^⊤ A 𝐱 - 2𝐱^⊤ 𝐛.\n",
    "$$.\n",
    "Of course we can use tried-and-true techniques implemented in `\\` but here we want\n",
    "to emphasise we can also solve this with simple optimsation algorithms like gradient desecent\n",
    "which do not know the structure of the problem. We consider a matrix where we know gradient descent\n",
    "will converge fast:\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 1/2^α \\\\ 1/2^α & 1 & ⋱ \\\\ &  ⋱ & ⋱ & 1/n^α \\\\ && 1/n^α & 1 \\end{bmatrix}\n",
    "$$\n",
    "In other words we want to minimise the functional (or the _loss function_)\n",
    "$$\n",
    "f_{A,𝐛}(𝐱) = ∑_{k=1}^n x_k^2 + ∑_{k=2}^n x_{k-1} x_k/k^α - ∑_{k=1}^n x_k b_k.\n",
    "$$\n",
    "For simplicity we will take $𝐛$ to be the vector with all ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Owing to the constraints of Zygote.jl, we need to write this in a vectorised way to ensure Zygote is sufficiently fast.\n",
    "Here we see that when we do this we can efficiently\n",
    "compute gradients even\n",
    "with a million degrees of freedom, way beyond what could ever be done with forward-mode automatic differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "x = randn(10)\n",
    "x'x == dot(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.5\n",
       " 0.3333333333333333\n",
       " 0.25\n",
       " 0.2"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 5; (2:n) .^ (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#203 (generic function with 1 method)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = function(x,α)\n",
    "    n = length(x)\n",
    "    x'x + 2(x[1:end-1] ./ (2:n) .^ α)'x[2:end] - 2sum(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.127429 seconds (67 allocations: 183.110 MiB, 69.29% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000-element Vector{Float64}:\n",
       " -2.156393415904757\n",
       " -0.45489682129103626\n",
       " -2.297094801172269\n",
       " -5.223394704340448\n",
       " -2.8524860041976927\n",
       " -4.437082578412204\n",
       " -0.3137381706675053\n",
       "  0.7064869386179122\n",
       " -2.177277589463353\n",
       " -5.94919199181359\n",
       " -2.71846722081614\n",
       " -0.4697837872474493\n",
       " -1.048064933452232\n",
       "  ⋮\n",
       " -3.929148823373159\n",
       " -3.493815702849622\n",
       " -1.2526140839739681\n",
       " -1.805076117190604\n",
       " -3.1385027035327067\n",
       " -1.5135972040700458\n",
       " -0.4539580277886235\n",
       " -2.5922998548466567\n",
       "  5.213551834822463\n",
       "  0.13930991398250625\n",
       " -0.65140110211543\n",
       " -2.6549540172874235"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1_000_000\n",
    "x = randn(n)\n",
    "@time gradient(f, x, 2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For concreteness we first implement our own version of a quick-and-dirty gradient descent:\n",
    "$$\n",
    "x_{k+1} = x_k - γ_k ∇f(x_k)\n",
    "$$\n",
    "where $γ_k$ is the learning rate. To choose $γ_k$ we just halve\n",
    "the learning rate until we see decrease in the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(γ, f(x, α)) = (9.5367431640625e-7, -999998.9059732673)\n",
      "(γ, f(x, α)) = (1.1920928955078125e-7, -999998.9059732673)\n",
      "(γ, f(x, α)) = (0.0009765625, -999998.9059732673)\n",
      "(γ, f(x, α)) = (0.5, -999998.9059732673)\n",
      "(γ, f(x, α)) = (0.015625, -999998.9059732673)\n",
      "(γ, f(x, α)) = (0.0078125, -999998.9059732673)\n",
      "(γ, f(x, α)) = (4.76837158203125e-7, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.384185791015625e-7, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9802322387695312e-8, -999998.9059732673)\n",
      "(γ, f(x, α)) = (1.4901161193847656e-8, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.3283064365386963e-10, -999998.9059732673)\n",
      "(γ, f(x, α)) = (5.820766091346741e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (5.820766091346741e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (5.820766091346741e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n",
      "(γ, f(x, α)) = (2.9103830456733704e-11, -999998.9059732673)\n"
     ]
    }
   ],
   "source": [
    "α = 2 # parameter in the functional \n",
    "for k = 1:30 # twenty steps of gradient descent\n",
    "    γ = 1 # learning rate\n",
    "    y = x - γ*gradient(f, x, α)[1]\n",
    "    while f(x,α) < f(y,α) # if we aren't decreasing\n",
    "        γ = γ/2 # halve learning rate\n",
    "        y = x - γ*gradient(f, x, α)[1]\n",
    "    end\n",
    "    x = y # x is the new y\n",
    "    @show γ, f(x,α)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this with the \"true\" solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice its better to use inbuilt optimsation routines and packages. Here we see how we can solve the same problem with\n",
    "the Optimization.jl package, combined with OptimizationOptimisers.jl that has gradient-based optimisation methods,\n",
    "in particular `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optimization # package for optimization\n",
    "using OptimizationOptimisers # loads gradient-based optimiser, eg Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.641426 seconds (22.29 k allocations: 23.285 GiB, 37.73% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000-element Vector{Float64}:\n",
       " 0.7926794970958331\n",
       " 0.7686944137269192\n",
       " 0.5155934708523282\n",
       " 0.9295595766056008\n",
       " 0.9336581485775208\n",
       " 0.9594261131093\n",
       " 0.9734772204953535\n",
       " 0.9745894495492411\n",
       " 0.9720664915796232\n",
       " 0.9830239649874682\n",
       " 0.9853045193982235\n",
       " 0.9810116907962902\n",
       " 0.9942975618395835\n",
       " ⋮\n",
       " 0.9600657763471684\n",
       " 1.0094952621275346\n",
       " 1.005434882999225\n",
       " 1.0030422125026865\n",
       " 1.0019575443279658\n",
       " 0.993896938682554\n",
       " 1.0006821822489984\n",
       " 0.9986364887878048\n",
       " 1.0021922858157455\n",
       " 0.9936826858773254\n",
       " 1.0076160939218697\n",
       " 0.9999614774136861"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1_000_000\n",
    "\n",
    "x = randn(n) # initial guess\n",
    "prob = OptimizationProblem(OptimizationFunction(f, AutoZygote()), x, α)\n",
    "@time y = solve(prob, Adam(0.03); maxiters=100)\n",
    "y.u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4** This problem considers an example that will connect with  neural networks.\n",
    "Define ${\\rm relu}(x) := \\max(0,x)$ and consider an approximation of the form:\n",
    "$$\n",
    "p_{𝐚,𝐛}(x) := ∑_{k=1}^n {\\rm relu}(a_k x + b_k)\n",
    "$$\n",
    "where $𝐚,𝐛 ∈ ℝ^n$. This is a sum of positive convex functions hence consider regression for a positive convex function\n",
    "like $f(x) =  \\exp x$. For $n = 100$,  approximate $𝐚,𝐛$ that minimises $\\|p_{𝐚,𝐛}.(𝐱) - f.(𝐱)\\|$ where $𝐱$ is a vector containing\n",
    "100 evenly spaced points between $-1$ and $1$ (inclusive). Compare your regression with $f$ by plotting the two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Construct a model for the function and perform regression using Optimization.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "myweirdfunction(x::Float64) = x^2\n",
    "Zygote.pullback(::typeof(myweirdfunction), x) = x, t -> 2x*t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
