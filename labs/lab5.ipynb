{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SciML SANUM2024\n",
    "# Lab 5: Neural Networks and Lux.jl\n",
    "\n",
    "In this lab we introduce neural networks as implemented in Lux.jl.\n",
    "A neural network (NN) is in some sense just a function with many parameters\n",
    "in a way that facilitates computing gradients with respect to these parameters.\n",
    "That is: it is at its core a way of book-keeping a heavily parameterised function.\n",
    "It is constructed by composing basic building blocks usually built from linear\n",
    "algebra operations, combined with simple _activator functions_.\n",
    "Here we look at the simplest case and see how the paremeters in a NN can be chosen to\n",
    "solve optimisation problems. In other words: we will _train_ the NN."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Learning Outcomes**\n",
    "1. Single-layer neural networks and activation functions.\n",
    "2. Creating deeper networks as a `Chain`.\n",
    "3. Training neural networks by simple optimisation."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Lux, Random, Optimization, OptimizationOptimisers, ComponentArrays, Zygote, Plots, LinearAlgebra, Test"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Single layer neural networks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin with a single-layer NN without an activator\n",
    "function which correspond to maps of the form:\n",
    "$$\n",
    "ùê± ‚Ü¶ Aùê± + ùêõ\n",
    "$$\n",
    "where $A ‚àà ‚Ñù^{m √ó n}$ and $ùêõ ‚àà ‚Ñù^n$. The space of such maps is\n",
    "modelled by the `Dense` type which has two paramters: `weight`, corresponding to $A$, and\n",
    " `bias`, corresponding to $ùêõ$. Here we see a simple example\n",
    "of constructing the model (the space of all such maps) and evaluating\n",
    "a specific map by specifying the parameters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note we have to pass an extra argument corresponding to\n",
    "the \"state\" of a NN: this doesn't exist for the simple layers we consider\n",
    "but more sophisticated NNs can depend on history and so the state records the relevant\n",
    "information."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m,n = 5,4\n",
    "\n",
    "model = Dense(n => m) # represents maps of the form ùê± ‚Ü¶ Aùê± + ùêõ\n",
    "\n",
    "A = randn(5,4)\n",
    "b = randn(5)\n",
    "x = randn(4)\n",
    "const NOSTATE = NamedTuple() # no state for our NN\n",
    "val,newst = model(x, (weight=A, bias=b), NOSTATE) # returns the output of the map and the updated state, which we ignore\n",
    "\n",
    "@test val == A*x + b # our model with these parameters is just A*x + b"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "An important feature is that we can compute gradients with respect to parameters of functions of our\n",
    "model. Before we looked at the case where\n",
    "we differentiated with respect to vectors but a powerful feature in Zygote is it works for other types, including the `NamedTuple`\n",
    "which Lux.jl uses for representing paramaters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ps = (weight=A, bias=b) # parameters as a NamedTuple, almost like an anonymous type\n",
    "ps_grad = gradient(p -> sum(model(x, p, NOSTATE)[1]), ps)[1] # returns a named tuple containing the gradients"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because our NN at this stage is linear in the paremeters the gradient is actually quite simple: eg the partial derivative with\n",
    "respect to $A[k,j]$ will just be $x[j]$ and the derivative with respect to $b[k]$ will just be $1$. Thus we get:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@test ps_grad.weight ‚âà ones(5) * x'\n",
    "@test ps_grad.bias ‚âà ones(5)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Going beyond basic linear algebra, we can apply an \"activator\" function $f$ to each\n",
    "entry of the map, to represent maps of the form:\n",
    "$$\n",
    "ùê± ‚Ü¶ f.(Aùê± + ùêõ)\n",
    "$$\n",
    "where we use the Julia-like broadcast notation to mean entrywise application.\n",
    "The classic activator is the `relu` function which is really just $\\max(0,x)$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = range(-1,1, 1000)\n",
    "plot(x, relu.(x); label=\"relu\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can incorporate this in our model as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = randn(4)\n",
    "model = Dense(4 => 5, relu)\n",
    "model(x, (weight = A, bias=b), NOSTATE)[1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we can compute gradients as before:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ps = (weight=A, bias=b)\n",
    "ps_grad = gradient(p -> sum(model(x, p, NOSTATE)[1]), ps)[1] # returns a named tuple containing the gradients"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 1** Derive the formula  for the gradient of the model with an activator function and compare it with\n",
    "the numerical result just computed. Hint: The answer depends on the output value."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "##¬†TODO: Compute the gradient by hand, matching ps_grad"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see an example directly related to a classic numerical analysis problem: approximating\n",
    "functions by a continuous piecewise affine\n",
    "function, as done in the Trapezium rule. Our model corresponds to a sum of weighted and shifted `relu` functions:\n",
    "$$\n",
    "p_{ùêö,ùêõ}(x) := ‚àë_{k=1}^n {\\rm relu}(a_k x + b_k)\n",
    "$$\n",
    "We note that this is a sum of positive convex functions so will only be useful for approximating positive convex functions\n",
    "(we will generalise this later).  Thus we want to choose the paremeters to fit data generated by a positive convex function,\n",
    "e.g., $f(x) = \\exp(x)$. Here we first generate \"training data\" which means the samples of the function on a grid."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 100\n",
    "x = range(-1, 1; length = n)\n",
    "y = exp.(x)\n",
    "plot(x, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our one-layer NN (before the summation) is\n",
    "$$\n",
    "  {\\rm relu}.(ùêöx + ùêõ)\n",
    "$$\n",
    "which corresponds to a simple dense layer with `relu` activation.\n",
    "We then sum over the output of this to get the model\n",
    "$$\n",
    "  [1,‚Ä¶,1]^‚ä§ {\\rm relu}.(ùêöx + ùêõ)\n",
    "$$\n",
    "In our case `x` is actually a vector containing the grid we sample on\n",
    "but we first need to transpose it to be a $1 √ó n$ matrix, which will apply the NN\n",
    "to each grid point. We can then sum over the columns to get the value of the model with the given\n",
    "parameters at the grid points."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nn = Dense(1 => n, relu)\n",
    "function summation_model(ps, (nn, x))\n",
    "    Y,st = nn(x', ps, NOSTATE) # k-th column contains relu.(ùêöx[k] + ùêõ)\n",
    "    vec(sum(Y; dims=1)) # sums over the columns\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to choose the parameters to minimise a loss function. Here we\n",
    "just wish to minimise the 2-norm error which we can write as follow:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function convex_regression_loss(ps, (nn, (x,y)))\n",
    "    yÃÉ = summation_model(ps, (nn, x))\n",
    "    norm(yÃÉ - y) # 2-norm error\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now setup the optimation problem. We can use `Lux.setup` to create a random initial guess for parameters\n",
    "though we need to supply a random number generator. We also need to wrap the returned named tuple in a `ComponentArray` as Optimization.jl\n",
    "requires the optimisation to be over an array-type."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rng = MersenneTwister() # Random number generator.\n",
    "ps = ComponentArray(Lux.setup(rng, nn)[1])\n",
    "\n",
    "prob = OptimizationProblem(OptimizationFunction(convex_regression_loss, Optimization.AutoZygote()), ps, (nn, (x, y)))\n",
    "@time ret = solve(prob, Adam(0.03), maxiters=250)\n",
    "\n",
    "plot(x, y)\n",
    "plot!(x, summation_model(ret.u, (nn, x)))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 2**  Replace `relu` in the activation function with a smooth `tanh` function and plot\n",
    "the result. Is the approximation as accurate? What if you increase the number of epochs?\n",
    "What if you construct your own function that is a smooth approximation to `relu`?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: setup a neural network with different activations"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Multiple layer neural networks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An effective NN will have more than one layer. A simple example is if we want to go beyond\n",
    "convex functions. Rather than simply summing over the NN we can allow different weights,\n",
    "giving us the model\n",
    "$$\n",
    "  ùêú^‚ä§ {\\rm relu}.(ùêöx + ùêõ) + d.\n",
    "$$\n",
    "Or we can think of $C = ùêú^‚ä§$ as a $1 √ó n$ matrix. This is in fact a composition of two simple layers, the first being\n",
    "$$\n",
    " x ‚Ü¶ {\\rm relu}.(ùêöx + ùêõ)\n",
    "$$\n",
    "and the second being one without an activation function:\n",
    "$$\n",
    " ùê± ‚Ü¶ C ùê± + d.\n",
    "$$\n",
    "I.e., they are both `Dense` layers just with different dimensions and different activation functions (`relu` and `identity`).\n",
    "We can create such a composition using the `Chain` command:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 100\n",
    "model = Chain(Dense(1 => n, relu), Dense(n => 1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here the parameters are nested. For example, we can create the relevant parameters as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ùêö,ùêõ = randn(n,1),randn(n)\n",
    "ùêú,d = randn(n),randn(1)\n",
    "\n",
    "st = (layer_1 = NOSTATE, layer_2 = NOSTATE) # each layer has its own state\n",
    "ps = (layer_1 = (weight = ùêö, bias = ùêõ), layer_2 = (weight = ùêú', bias = d))\n",
    "\n",
    "@test model([0.1], ps, st)[1] ‚âà ùêú'*relu.(ùêö*0.1 + ùêõ) + d"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can plot the model evaluated at the gri to see that it is indeed (probably) no longer convex:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(x, vec(model(x', ps, st)[1]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now choose the parameters to fit data. Let's generate data for a non-convex function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = range(-1, 1; length = n)\n",
    "y = sin.(3x).*exp.(x)\n",
    "plot(x,y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will fit this data by minimising the 2-norm with a different model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function regression_loss(ps, (model, st, (x,y)))\n",
    "    yÃÉ = vec(model(x', ps, st)[1])\n",
    "    norm(yÃÉ - y) # 2-norm error\n",
    "end\n",
    "\n",
    "ps,st = Lux.setup(rng, model)\n",
    "prob = OptimizationProblem(OptimizationFunction(regression_loss, Optimization.AutoZygote()), ComponentArray(ps), (model, st, (x, y)))\n",
    "@time ret = solve(prob, Adam(0.03), maxiters=250)\n",
    "\n",
    "plot(x,y)\n",
    "plot!(x, vec(model(x', ret.u, st)[1]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    " It does OK but is still not particularly impressive. The real power in neural networks is their approximation power\n",
    "increases as we add more layers. Here let's try an example with 3-layers."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Chain(Dense(1 => 200, relu), Dense(200 => 200, relu), Dense(200 => 1))\n",
    "\n",
    "ps,st = Lux.setup(rng, model)\n",
    "prob = OptimizationProblem(OptimizationFunction(regression_loss, Optimization.AutoZygote()), ComponentArray(ps), (model, st, (x, y)))\n",
    "@time ret = solve(prob, Adam(0.03), maxiters=1000)\n",
    "\n",
    "plot(x,y)\n",
    "plot!(x, vec(model(x', ret.u, st)[1]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 3** Add a 4th layer and 5th layer, but not all involving square matrices.\n",
    "Can you choose the size of the layers and the activation functions to\n",
    "match the eyeball norm? Hint: the answer might be \"no\" üòÖ But maybe \"ballpark norm\" is sufficient."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: Setup a NN with 4 and 5 layers."
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
