{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SciML SANUM2024\n",
    "# Lab 3: Reverse-mode automatic differentiation and Zygote.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the number of unknowns becomes large forward-mode automatic differentiation as\n",
    "implemented in ForwardDiff.jl becomes prohibitively expensive for computing gradients and instead we need to\n",
    "use reverse-mode automatic differentiation: this is best thought of as implementing the chain-rule\n",
    "in an automatic fashion, with a specific choice of multiplying the underlying Jacobians."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Computing gradients is important for solving optimisation problems, which is what ultimately what training a neural network\n",
    "is. Therefore we also look at solving some\n",
    "simple optimissation problems, using Optimsation.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Learning Outcomes**\n",
    "\n",
    "1. Computing gradients and derivatives with Zygote.jl\n",
    "2. Basics of reverse-mode automatic differentiation and pullbacks.\n",
    "3. Forward-mode automatic differentiation via pushforwards.\n",
    "4. Using automatic differentiation for implementing gradient descent.\n",
    "5. Solving optimisation with gradient descent and via Optimsation.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Using Zygote.jl for differentiation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin with a simple demonstration of Zygote.jl, which can be thought of as a replacement for ForwardDiff.jl that\n",
    "uses reverse-mode differentiation under the hood. We can differentiate scalar functions, but unlike ForwardDiff.jl it\n",
    "overloads the `'` syntax to mean differentiation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The real power of Zygote.jl is computing gradients (or more generally, Jacobians\n",
    "of $f : ℝ^m → ℝ^n$ where $n ≪ m$). We can compute a gradient of the function we considered before as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f = function(x)\n",
    "    ret = zero(eltype(x))\n",
    "    for k = 1:length(x)-1\n",
    "        ret += x[k]*x[k+1]\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unlike ForwardDiff.jl, the gradient returns a tuple since multiple arguments are supported in addition\n",
    "to vector inputs, eg:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now differentiating this function is not particularly faster than ForwardDiff.jl:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It also uses more memory the larger the computation. Take for example\n",
    "the Taylor series for the exponential from Lab 1:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function exp_t(z, n)\n",
    "    ret = 1.0\n",
    "    s = 1.0\n",
    "    for k = 1:n\n",
    "        s = s/k * z\n",
    "        ret = ret + s\n",
    "    end\n",
    "    ret\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The more terms we take the more memory is used, despite the function itself\n",
    "using no memory:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another catch is Zygote.jl doesn't support functions that mutate arrays. Here's an example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is unlike `ForwardDiff.gradient` which works fine for differentiating `f!`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So why do we use reverse-mode automatic differentiation when it has so many weaknesses\n",
    "compared to forward-mode?\n",
    "Because if we write code in just the right way it becomes extremely fast.\n",
    "For example, if we rewrite `f` in a vectorised form we see a huge improvement over\n",
    "ForwardDiff.jl:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Conclusion**: Zygote.jl is much more brittle, sometimes fails outright, requires\n",
    "writing functions in a specific way, uses a lot more memory to record complicated operations, but when it works\n",
    "well it is _extremely_ fast. Thus when we get to neural networks it is paramount that\n",
    "we design our representations of neural networks in a way that is ameniable to reverse-mode\n",
    "automatic differentiation, as implemented in Zygote.jl."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Pullbacks and back-propagation for scalar functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now peek a little under-the-hood to get some intuition on how Zygote.jl is computing\n",
    "derivatives, and to understand why its so much faster than ForwardDiff.jl in certain situations. Underlying automatic\n",
    "differentiation in Zygote.jl are so-called \"pullback\"s. In the scalar\n",
    "case these are very close to the notion of a derivative. However, rather than\n",
    "the derivative being a single constant, it's a linear map representing the derivative:\n",
    "eg, if the derivative of $f(x)$ is denoted $f'(x)$ then the pullback is a linear map\n",
    "$$\n",
    "t ↦ f'(x)t.\n",
    "$$\n",
    "We can compute pullbacks using the `pullback` routine:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`p_sin` contains the map $t ↦ \\cos(0.1) t$. Since pullbacks support multiple arguments\n",
    "it actually returns a tuple with a single entry:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus to get out the value we use the following:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The reason its a map instead of just a scalar becomes important for the vector-valued case\n",
    "where Jacobians can often be applied to vectors much faster than constructing the Jacobian matrix and\n",
    "performing a matrix-vector multiplication."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pullbacks can be used for determining more complicated derivatives. Consider a composition of three functions\n",
    "$h ∘ g ∘ f$ where from the Chain Rule we know:\n",
    "$$\n",
    "{{\\rm d} \\over {\\rm d} x}[f(g(h(x))] = f'(g(h(x)) g'(h(x)) h'(x)\n",
    "$$\n",
    "Essentially we have three pullbacks: the first is the pullback of $f$ evaluated\n",
    "at $x$, the second corresponding to $g$ evaluated at $f(x)$, and the third\n",
    "corresponding to $h$ evaluated at $g(f(x))$, that is:\n",
    "$$\n",
    "\\begin{align*}\n",
    " p_1(t) &= f'(x) t  \\\\\n",
    " p_2(t) &= g'(f(x)) t  \\\\\n",
    " p_3(t) &= h'(g(f(x))t\n",
    "\\end{align*}\n",
    "$$\n",
    "Thus the derivative is given by either the _forward_ or _reverse_ composition of these functions:\n",
    "$$\n",
    " p_3(p_2(p_1(1))) = p_1(p_2(p_3(1))) = h'(g(f(x))g'(f(x))f'(x).\n",
    "$$\n",
    "The first version is called _forward-propagation_ and the second called _back-propagation_.\n",
    "Forward-propagation is a version of forward-mode automatic differentiation and is essentially equivalent to using dual numbers.\n",
    "We will see later in the vector case that forward- and back-propagation are not the same,\n",
    "and that back-propagation is much more efficient provided the output is scalar (or small dimensional)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see pullbacks in action for computing the derivative of $\\cos\\sqrt{{\\rm e}^x}$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see how this can lead to an approach for automatic differentiation.\n",
    "For example, consider the following function composing `sin` over and over:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function manysin(n, x)\n",
    "    r = x\n",
    "    for k = 1:n\n",
    "        r = sin(r)\n",
    "    end\n",
    "    r\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we would need `n` pullbacks as each time `sin` is called at a different value.\n",
    "But the number of such pullbacks grows only linearly so this is acceptable. So thus\n",
    "at a high-level we can think of Zygote as running through and computing all the pullbacks:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To deduce the derivative we need can either do forward- or back-propogation by looping through our pullbacks\n",
    "either in forward- or in reverse-order. Here we implement back-propagation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zygote constructs code that is equivalent to this loop automatically,\n",
    "constructing a high-performance version of this back-propogation loop at compile time using something called source-to-source\n",
    "differentiation. But there's no getting around the fact that it needs to record the pullbacks so it does use more memory the larger\n",
    "the computation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 1** Compute the derivative of `manysin` using forward-propagation, by looping through the pull-backs\n",
    "in the forward direction."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: loop through pullbacks in order to compute the derivative."
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Pullbacks with multiple arguments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Things become more complicated when we have a function with multiple arguments, even in the\n",
    "scalar case. Consider now the function $f(g(x), h(x))$. The chain rule tells us that\n",
    "$$\n",
    "{{\\rm d} \\over {\\rm d} x}[f(g(x), h(x))] = f_x(g(x), h(x)) g'(x) + f_y(g(x), h(x)) h'(x)\n",
    "$$\n",
    "Now we have three pullbacks:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_1(t) &= g'(x) t\\\\\n",
    "p_2(t) &= h'(x) t\\\\\n",
    "p_3(t) &= [f_x(g(x), h(x))t, f_y(g(x), h(x))t]\n",
    "\\end{align*}\n",
    "$$\n",
    "In this case the derivative can be recovered via back-propagation via:\n",
    "$$\n",
    "p_1(p_3(1)[1]) + p_2(p_3(1)[2]).\n",
    "$$\n",
    "Here we see a simple example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Doing more complicated calculations or indeed algorithms becomes\n",
    "quite complicated if there are interdependencecies, eg, $f(g(r(x)), h(r(x)))$.\n",
    "This explains why our first version of a function summing over products of its arguments\n",
    "was so slow.\n",
    "Fortunately, there is an alternative: we can focus on composing vector functions.\n",
    "Eg, such a function can be thought of as composition:\n",
    "$$\n",
    "f ∘ 𝐠 ∘ r\n",
    "$$\n",
    "where $𝐠(x) = [g(x),h(x)]$. This is a special case of what we discuss in the next section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Gradients and pullbacks\n",
    "\n",
    "Now we consider computing gradients of functions that are compositions\n",
    "of vector functions, which neural networks fall into.\n",
    "Again, we denote the Jacobian as\n",
    "$$\n",
    " J_f = \\begin{bmatrix} {∂ f_1 \\over ∂x_1} & ⋯ & {∂ f_1 \\over ∂x_ℓ} \\\\\n",
    "      ⋮ & ⋱ & ⋮ \\\\\n",
    "      {∂ f_m \\over ∂x_1} & ⋯ & {∂ f_m \\over ∂x_ℓ}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that gradients are the transpose of Jacobians: $∇h = J_h^⊤$.\n",
    "For a scalar-valued function $f : ℝ^n → ℝ$ the pullback represents the linear map\n",
    "$p_{f,𝐱} : ℝ → ℝ^n$ corresponding to scaling the gradient:\n",
    "$$\n",
    "p_{f,𝐱}(t) = J_f(𝐱)^⊤t = ∇f(𝐱) t\n",
    "$$\n",
    "Here we see an example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a function $f : ℝ^n → ℝ^m$ the the pullback represents the linear map $p_{f,𝐱} : ℝ^m → ℝ^n$ given by\n",
    "$$\n",
    "p_{f,𝐱}(t) = J_f(𝐱)^⊤𝐭\n",
    "$$\n",
    "Here is a simple example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider a composition $f : ℝ^n → ℝ^m$, $g : ℝ^m → ℝ^ℓ$ and $h : ℝ^ℓ → ℝ$, that is,\n",
    "we want to compute the gradient of $h ∘ g ∘ f : ℝ^n → ℝ$. The Chain rule tells us that\n",
    "$$\n",
    " J_{h ∘ g ∘ f}(𝐱) = J_h(g(f(𝐱)) J_g(f(𝐱)) J_f(𝐱)\n",
    "$$\n",
    "Put another way, the gradiant of $h ∘ g ∘ f$\n",
    "is given by the transposes of Jacobians:\n",
    "$$\n",
    "   ∇[{h ∘ g ∘ f}](𝐱) = J_f(𝐱)^⊤ J_g(f(𝐱))^⊤  ∇h(g(f(𝐱))\n",
    "$$\n",
    "Thus we have three pullbacks $p_1 : ℝ^m → ℝ^n$, $p_2 : ℝ^ℓ → ℝ^m$ and $p_3 : ℝ → ℝ^ℓ$ given by\n",
    "\\begin{align*}\n",
    " p_1(𝐭) &= J_f(𝐱)^⊤ 𝐭  \\\\\n",
    " p_2(𝐭) &= J_g(f(x))^⊤ 𝐭  \\\\\n",
    " p_3(t) &= ∇h(g(f(𝐱)) t\n",
    "\\end{align*}\n",
    "The gradient is given by _back-propagation_:\n",
    "$$\n",
    " p_1(p_2(p_3(1))) = J_f(𝐱)^⊤ J_g(f(𝐱))^⊤  ∇h(g(f(𝐱)).\n",
    "$$\n",
    "Here the \"right\" order to do the multiplications is clear: matrix-matrix multiplications are expensive\n",
    "so its best to do it reverse order so that we only ever have matrix-vector multiplications.\n",
    "Also, the pullback doesn't give us enough information to implement forward-propagation:\n",
    "we don't have access to the Jacobian matrices, or their application."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an example consider computing the gradient of an iteration a simple map like:\n",
    "$$\n",
    "𝐟(x,y,z) = \\begin{bmatrix} \\cos(xy)+z\\\\ zy-\\exp(x)\\\\ x + y + z \\end{bmatrix}\n",
    "$$\n",
    "and summing over the result, eg. computing $[1,1,1]^⊤(\\underbrace{𝐟 ∘ ⋯ ∘ 𝐟}_{n\\hbox{ times}})(𝐱)$.\n",
    "We implement this with a general function `iteratef`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get an idea how this works behind the scenes we can again accumulate the pullbacks:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can recover the gradient by back-propogation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indeed we match the gradient as computed with Zygote.jl:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 2** The function `pushforward` represent the map $𝐭 ↦ J_f(𝐱) 𝐭$.\n",
    "Compute the gradient of `iteratef` as above with forward-mode automatic differentiation by using `pushforward`.\n",
    "Do so without creating a vector of pushforwards.\n",
    "Hint: We need to run the pushforward iteration with the identity matrix as the initial value,\n",
    "but the result of  `pushforward` only works on vectors. So we need to apply it to each column of the matrix manually."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: Compute the gradient as above but using pushforward"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 3** Consider a simple forward Euler method approximating the solution to the Pendulum equation with friction:\n",
    "$$\n",
    "u'' = τ u' - \\sin u\n",
    "$$\n",
    "which we can rewrite as a first order system:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   u' \\\\\n",
    "   v'\n",
    "   \\end{bmatrix} = \\begin{bmatrix} v \\\\ -τ*v - \\sin u \\end{bmatrix}\n",
    "$$\n",
    "That is, we want to implement the iteration\n",
    "$$\n",
    "𝐮_{k+1} = 𝐮_k + h*\\begin{bmatrix} 𝐮_k[2] \\\\ -τ 𝐮_k[2] - \\sin 𝐮_k[1] \\end{bmatrix}\n",
    "$$\n",
    "with a specified initial condition $𝐮_0$. For $N = 100$, $h = 0.1$ and $𝐮_0 = [0.1,0.2]$, differentiate\n",
    "the solution with-respect to $τ$ at $τ = 1$ by creating a vector of pullbacks and implementing back-propagation.\n",
    "Hint: Forward Euler is a variant of `iteratef` above so you can modify the subsequent pullback construction. Add $τ$ to the vector\n",
    "of values to capture the relevant dependencies and verify your result by comparing to `gradient`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 Optimisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A key place where reverse-mode automatic differentiation is essential is large scale optimisation.\n",
    "As a  simple example we will look at the classic optimisation problem\n",
    "that solves $A 𝐱 = 𝐛$ where $A$ is symmetric positive definite: find $𝐱$ that minimises\n",
    "$$\n",
    "f_{A,𝐛}(𝐱) = 𝐱^⊤ A 𝐱 - 2𝐱^⊤ 𝐛.\n",
    "$$.\n",
    "Of course we can use tried-and-true techniques implemented in `\\` but here we want\n",
    "to emphasise we can also solve this with simple optimsation algorithms like gradient desecent\n",
    "which do not know the structure of the problem. We consider a matrix where we know gradient descent\n",
    "will converge fast:\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 1/2^α \\\\ 1/2^α & 1 & ⋱ \\\\ &  ⋱ & ⋱ & 1/n^α \\\\ && 1/n^α & 1 \\end{bmatrix}\n",
    "$$\n",
    "In other words we want to minimise the functional (or the _loss function_)\n",
    "$$\n",
    "f_{A,𝐛}(𝐱) = ∑_{k=1}^n x_k^2 + ∑_{k=2}^n x_{k-1} x_k/k^α - ∑_{k=1}^n x_k b_k.\n",
    "$$\n",
    "For simplicity we will take $𝐛$ to be the vector with all ones."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Owing to the constraints of Zygote.jl, we need to write this in a vectorised way to ensure Zygote is sufficiently fast.\n",
    "Here we see that when we do this we can efficiently\n",
    "compute gradients even\n",
    "with a million degrees of freedom, way beyond what could ever be done with forward-mode automatic differentiation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For concreteness we first implement our own version of a quick-and-dirty gradient descent:\n",
    "$$\n",
    "x_{k+1} = x_k - γ_k ∇f(x_k)\n",
    "$$\n",
    "where $γ_k$ is the learning rate. To choose $γ_k$ we just halve\n",
    "the learning rate until we see decrease in the loss function."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compare this with the \"true\" solution:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In practice its better to use inbuilt optimsation routines and packages. Here we see how we can solve the same problem with\n",
    "the Optimization.jl package, combined with OptimizationOptimisers.jl that has gradient-based optimisation methods,\n",
    "in particular `Adam`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 4** This problem considers an example that will connect with  neural networks.\n",
    "Define ${\\rm relu}(x) := \\max(0,x)$ and consider an approximation of the form:\n",
    "$$\n",
    "p_{𝐚,𝐛}(x) := ∑_{k=1}^n {\\rm relu}(a_k x + b_k)\n",
    "$$\n",
    "where $𝐚,𝐛 ∈ ℝ^n$. This is a sum of positive convex functions hence consider regression for a positive convex function\n",
    "like $f(x) =  \\exp x$. For $n = 100$,  approximate $𝐚,𝐛$ that minimises $\\|p_{𝐚,𝐛}.(𝐱) - f.(𝐱)\\|$ where $𝐱$ is a vector containing\n",
    "100 evenly spaced points between $-1$ and $1$ (inclusive). Compare your regression with $f$ by plotting the two functions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: Construct a model for the function and perform regression using Optimization.jl"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
