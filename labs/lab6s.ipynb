{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SciML SANUM2024\n",
    "# Lab 6: Neural Differential Equations and DiffEqFlux.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this final lab we look at combining differential equations and\n",
    "neural networks, with the goal of \"learning\" dynamics based on training data.\n",
    "That is, consider an ODE of the form\n",
    "$$\n",
    "u' = f(u) + g(u)\n",
    "$$\n",
    "where we know $f$ (or if we don't know anything, $f = 0$) but don't know\n",
    "$g$. We can approximate $g$ by a neural network, and then we want to choose\n",
    "the parameters to fit data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we look at some simple examples, but the same techniques have been used\n",
    "in clinical trial accelleration for vaccine development by Moderna,\n",
    "climate change modelling and COVID prediction, see the [SciML Schowcase](https://sciml.ai/showcase/)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Lux, DifferentialEquations, Optimization, OptimizationOptimisers, Plots, Zygote, SciMLSensitivity, ComponentArrays, Random, LinearAlgebra, Test"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1 Learning dynamics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin with a very simple ODE:\n",
    "$$\n",
    "u' = u - α u^3\n",
    "$$\n",
    "where we know $f(u) = u$ but suppose we don't know $g(u) = -α u^2$.\n",
    "First let's setup some training data with different initial conditions.\n",
    "We will do 10 trials which are sampled at 15 points for $t ∈ [0,5]$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function firstorder_rhs!(du, u, α, t)\n",
    "    du[1] = u[1] - α*u[1]^3\n",
    "end\n",
    "\n",
    "# Provide a random number generator for reliability (and so data covers large range of possible $u$ values)\n",
    "rng =  MersenneTwister(2121)\n",
    "α = 2.3 # arbitrary scaling\n",
    "N_trials = 15\n",
    "t = range(0, 5; length=15)\n",
    "data = zeros(length(t), N_trials)\n",
    "for j = 1:N_trials\n",
    "    u₀ = randn(rng) # random initial condition\n",
    "    prob = ODEProblem(firstorder_rhs!, [u₀], (0.0, t[end]), α)\n",
    "    data[:,j] = Vector(solve(prob; saveat=t))\n",
    "end\n",
    "\n",
    "scatter(t, data; legend=false) # plot the data"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now try to deduce the term $-αu^3$ by training a simple NN\n",
    "by minimising the error when comparing the model to the provided data.\n",
    "Because Optimzation.jl (currently) requires that parameters behave like\n",
    "arrays, rather than passing in the NN as a parameter we make it\n",
    "a global constant. We begin with simple 2-layer piecewise affine NN:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 100\n",
    "const RELU_MODEL = Chain(Dense(1 => n, relu), Dense(n => 1))\n",
    "\n",
    "ps,st = Lux.setup(rng, RELU_MODEL)\n",
    "const RELU_ST = st # RELU_ST is \"no state\", make it a constant\n",
    "ps = ComponentArray(ps); # Convert our parameters to an AbstractArray"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our model is\n",
    "$$\n",
    "  u' = u + g(u)\n",
    "$$\n",
    "where we represent $g$ by a NN with given parameters. Here is the rhs for this simple model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function firstorder_rhs_nn!(du, u, p, t)\n",
    "    du[1] = u[1]  + RELU_MODEL(u, p, RELU_ST)[1][1]\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then compute the loss by solving the ODE with a given set of parameters\n",
    "for each of the runs in our samples and summing over the 2-norms of the error\n",
    "between our prediction and the data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function firstorder_loss(p, (data, t))\n",
    "    loss = 0.0\n",
    "    for j = 1:size(data,2)\n",
    "        prob = ODEProblem(firstorder_rhs_nn!, data[1:1,j], (0.0, t[end]), p)\n",
    "        pred = solve(prob, Vern7(), abstol = 1e-6, reltol = 1e-6, saveat=t)\n",
    "        loss += norm(data[:,j] - Vector(pred))\n",
    "    end\n",
    "    loss\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are now ready to optimise. This will take some time so to avoid boredom\n",
    "and to understand how well the optimisation is working we will plot the\n",
    "model prediction of $g$ as we run the optimiser. To do this we provide\n",
    "a simple callback. This probably slows down the optimisation but is useful\n",
    "for us to see, and probably useful in practice to tell when the optimisation is\n",
    "stuck:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "relu_callback = function (p, l)\n",
    "    g = range(-1,1;length=30)\n",
    "    pred =  RELU_MODEL(g', p.u, RELU_ST)[1]'\n",
    "    plt = plot(g, -2.3*g.^3; label=\"true\")\n",
    "    plot!(plt, g, pred; label = \"prediction\", title=\"loss: $l\")\n",
    "    display(plt)\n",
    "    return false\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now setup the optimisation and run it 200 times:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prob = OptimizationProblem(OptimizationFunction(firstorder_loss, AutoZygote()), ps, (data, t))\n",
    "@time ret = solve(prob, Adam(0.03), maxiters=200, callback=relu_callback)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We didn't do very well. Let's try changing the optimiser, passing in the previous solution\n",
    "as the initial guess:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using OptimizationOptimJL # Load LBFGS optimiser\n",
    "prob = OptimizationProblem(OptimizationFunction(firstorder_loss, AutoZygote()), ret.u, (data, t))\n",
    "@time ret = solve(prob, LBFGS(), maxiters=200, callback=relu_callback)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This did much better and meets the ballpark norm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 1** Replace the neural network with a multilayer network and smooth activation\n",
    "function. Can you get better results than the simple RELU network?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: Construct a multilayer NN with smooth activation and see if it performs better\n",
    "\n",
    "# SOLUTION\n",
    "\n",
    "smoothstep = x -> x*(tanh(10x) + 1)/2\n",
    "\n",
    "# Multilayer model\n",
    "const SMOOTHSTEP_MODEL = Chain(Dense(1, 5, smoothstep), Dense(5, 5, smoothstep), Dense(5, 5, smoothstep),\n",
    "              Dense(5, 1))\n",
    "# Get the initial parameters and state variables of the model\n",
    "ps, st = Lux.setup(rng, SMOOTHSTEP_MODEL); ps = ComponentArray(ps)\n",
    "const SMOOTHSTEP_ST = st\n",
    "\n",
    "function firstorder_rhs_smoothstep!(du, u, p, t)\n",
    "    du[1] = u[1]  + SMOOTHSTEP_MODEL(u, p, SMOOTHSTEP_ST)[1][1]\n",
    "end\n",
    "\n",
    "function firstorder_loss_smoothstep(p, (data, t))\n",
    "    loss = 0.0\n",
    "    for j = 1:size(data,2)\n",
    "        prob = ODEProblem(firstorder_rhs_smoothstep!, data[1:1,j], (0.0, t[end]), p)\n",
    "        pred = Vector(solve(prob, Vern7(), abstol = 1e-6, reltol = 1e-6, saveat=t))\n",
    "        loss += norm(data[:,j] - pred)\n",
    "    end\n",
    "    loss\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "smoothstep_callback = function (p, l)\n",
    "    g = range(-1,1;length=30)\n",
    "    pred =  SMOOTHSTEP_MODEL(g', p.u, SMOOTHSTEP_ST)[1]'\n",
    "    plt = plot(g, -2.3*g.^3; label=\"true\")\n",
    "    plot!(plt, g, pred; label = \"prediction\", title=\"loss: $l\")\n",
    "    display(plt)\n",
    "    return false\n",
    "end\n",
    "\n",
    "prob = OptimizationProblem(OptimizationFunction(firstorder_loss_smoothstep, AutoZygote()), ps, (data, t))\n",
    "@time ret = solve(prob, Adam(0.03), maxiters=300, callback=smoothstep_callback)\n",
    "\n",
    "prob = OptimizationProblem(OptimizationFunction(firstorder_loss_smoothstep, AutoZygote()), ret.u, (data, t))\n",
    "@time ret = solve(prob, LBFGS(), maxiters=200, callback=smoothstep_callback)\n",
    "\n",
    "prob = OptimizationProblem(OptimizationFunction(firstorder_loss_smoothstep, AutoZygote()), ret.u, (data, t))\n",
    "@time ret = solve(prob, LBFGS(), maxiters=200, callback=smoothstep_callback)\n",
    "\n",
    "# I can't get better results!  😅\n",
    "\n",
    "# END"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 2** Use the predator-prey model\n",
    "$$\n",
    "\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} =  \\begin{bmatrix}αx - βxy \\\\  δxy - γy\\end{bmatrix}\n",
    "$$\n",
    "on $T ∈ [0,5]$ with $α , β,δ,γ = 1,2,3,4$ with initial condition $[1,2]$\n",
    "to generate training data of samples at 21 evenly spaced points (only do a single run).\n",
    "Suppose we do not know the whole interaction but can model\n",
    "$$\n",
    " \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} =  \\begin{bmatrix}αx \\\\ - γy\\end{bmatrix} + g(x,y)\n",
    "$$\n",
    "where $g :ℝ^2 → ℝ^2$ is modeled by a Neural Network. Deduce $g$ by optimization of a loss when\n",
    "compared to the training data.\n",
    "Hint: This [SciML example](https://docs.sciml.ai/Overview/stable/showcase/missing_physics/)\n",
    "solves this problem and might help guide you."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: Learn the dynamics in a predator-prey model.\n",
    "\n",
    "# SOLUTION\n",
    "# This is modified from the above link.\n",
    "\n",
    "function lotka!(du, u, p, t)\n",
    "    x,y = u\n",
    "    α, β, γ, δ = p\n",
    "    du[1] = α * x - β * y * x\n",
    "    du[2] = γ * x * y - δ * y\n",
    "end\n",
    "\n",
    "# Define the experimental parameter\n",
    "u₀ = [1,2]\n",
    "p_ = [1,2,3,4]\n",
    "prob = ODEProblem(lotka!, u0, (0.0, 5.0), p_)\n",
    "t = range(0, 5; length=21)\n",
    "solution = solve(prob, Vern7(), abstol = 1e-12, reltol = 1e-12, saveat = t)\n",
    "plot(solution)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X = Array(solution)\n",
    "\n",
    "rbf(x) = exp.(-(x .^ 2))\n",
    "\n",
    "# Multilayer FeedForward\n",
    "const RBF_MODEL = Chain(Dense(2, 5, rbf), Dense(5, 5, rbf), Dense(5, 5, rbf), Dense(5, 2))\n",
    "# Get the initial parameters and state variables of the model\n",
    "ps, st = Lux.setup(rng, RBF_MODEL); ps = ComponentArray(ps)\n",
    "const RBF_ST = st\n",
    "\n",
    "# Define the hybrid model\n",
    "function ude_dynamics_rhs!(du, u, p, t)\n",
    "    û = RBF_MODEL(u, p, RBF_ST)[1] # Network prediction\n",
    "    du[1] = u[1] + û[1]\n",
    "    du[2] = -2u[2] + û[2]\n",
    "end\n",
    "\n",
    "# Define the problem\n",
    "\n",
    "function ude_solve(p)\n",
    "    prob = ODEProblem(ude_dynamics_rhs!, [1.,2.], (0.0, 5.0), p)\n",
    "    solve(prob, Vern7(), abstol = 1e-6, reltol = 1e-6, saveat=t)\n",
    "end\n",
    "\n",
    "function ude_loss(p, (data, t))\n",
    "    pred = Array(ude_solve(p))\n",
    "    norm(data - pred)\n",
    "end\n",
    "\n",
    "\n",
    "ude_callback = function (p, l)\n",
    "    display(plot(ude_solve(p.u); title=\"loss = $l\"))\n",
    "    return false\n",
    "end\n",
    "\n",
    "prob = OptimizationProblem(OptimizationFunction(ude_loss, AutoZygote()), ps, (X, t))\n",
    "@time ret = solve(prob, Adam(0.03), maxiters=5000, callback=ude_callback)\n",
    "\n",
    "prob = OptimizationProblem(OptimizationFunction(ude_loss, AutoZygote()), ret.u, (X, t))\n",
    "@time ret = solve(prob, LBFGS(), maxiters=2000, callback=ude_callback)\n",
    "\n",
    "\n",
    "# END"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
