{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SciML SANUM2024\n",
    "# Lab 4: Neural Networks and Lux.jl\n",
    "\n",
    "In this lab we introduce neural networks as implemented in Lux.jl.\n",
    "A neural network (NN) is in some sense just a function with many parameters\n",
    "in a way that facilitates computing gradients with respect to these parameters.\n",
    "That is: it is at its core a way of book-keeping a heavily parameterised function.\n",
    "It is constructed by composing basic building blocks usually built from linear\n",
    "algebra operations, combined with simple _activator functions_.\n",
    "Here we look at the simplest case and see how the paremeters in a NN can be chosen to\n",
    "solve optimisation problems."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Learning Outcomes**\n",
    "1. Single-layer neural networks and activation functions.\n",
    "2. Creating deeper networks as a `Chain`.\n",
    "3. Training neural networks by simple optimisation."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Lux, Random, Optimisers, Zygote, Plots, LinearAlgebra, Test"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Single layer neural networks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin with a single-layer neural network without an activator\n",
    "function. This happens to be precisely maps of the form\n",
    "$$\n",
    "ùê± ‚Ü¶ Aùê± + ùêõ\n",
    "$$\n",
    "where $A ‚àà ‚Ñù^{m √ó n}$ and $ùêõ ‚àà ‚Ñù^n$. The space of such maps is\n",
    "modelled by the `Dense` type, where the `weight` corresponds to $A$\n",
    "and the `bias` corresponds to $ùêõ$. Here we see a simple example\n",
    "of constructing the model (the space of all such maps) and evaluating\n",
    "a specific map by specifying the paramters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m,n = 5,4\n",
    "\n",
    "model = Dense(n => m) # represents\n",
    "\n",
    "A = randn(5,4)\n",
    "b = randn(5)\n",
    "x = randn(4)\n",
    "const NOSTATE = NamedTuple() # no state\n",
    "val,newst = model(x, (weight=A, bias=b), NOSTATE) # returns the output of the map and the \"state\", which we ignore\n",
    "\n",
    "@test val ‚âà A*x + b # our model with these parameters is just A*x + b"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "An important feature is that we can compute gradients with respect to parameters of functions of our\n",
    "model. Before we looked at the case where\n",
    "we differentiated with respect to vectors but a power feature in Zygote is it works for all types like named-tuples."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ps = (weight=A, bias=b)\n",
    "ps_grad = gradient(p -> sum(model(x, p, NOSTATE)[1]), ps)[1] # returns a named tuple containing the gradients"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because our Neural Network at this stage is linear in the paremeters the gradient is actually quite simple: eg the partial derivative with\n",
    "respect to $A[k,j]$ will just be $x[j]$ and the derivative with respect to $b[k]$ will just be $1$. Thus we get:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@test ps_grad[:weight] ‚âà ones(5) * x'\n",
    "@test ps_grad[:bias] ‚âà ones(5)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Going beyond basic linear algebra, we can apply an \"activator\" function $f$ to each\n",
    "entry of the map, to represent maps of the form:\n",
    "$$\n",
    "ùê± ‚Ü¶ f.(Aùê± + ùêõ)\n",
    "$$\n",
    "Where we use the Julia-like broadcast notation to mean entrywise application.\n",
    "The classic in ML is the `relu` function which is really just $\\max(0,x)$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = range(-1,1, 1000)\n",
    "plot(x, relu.(x); label=\"relu\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can incorporate this in our model as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Dense(4 => 5, relu)\n",
    "model(x, (weight = A, bias=b), NOSTATE)[1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we can differentiate:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ps = (weight=A, bias=b)\n",
    "ps_grad = gradient(p -> sum(model(x, p, NOSTATE)[1]), ps)[1] # returns a named tuple containing the gradients"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 1** Derive the forumula  for the gradient of the model with an activator function and compare it with\n",
    "the numerical result just computed. Hint: The answer depends on the output value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see an example directly related to a classic numerical analysis problem: approximating\n",
    "functions by a continuous piecewise affine\n",
    "function, as done in the Trapezium rule. Our model corresponds to a sum of weighted and shifted `relu` functions:\n",
    "$$\n",
    "p_{ùêö,ùêõ}(x) := ‚àë_{k=1}^n {\\rm relu}(a_k x + b_k)\n",
    "$$\n",
    "We note that this is a sum of positive convex functions so only useful for approximating positive convex functions\n",
    "(we will generalise this later).  Thus we want to choose the paremeters to fit data generated by a positive convex function,\n",
    "e.g., $f(x) = 1 - \\exp(-x^2)$. Here we first generate \"training data\" which means the samples of the function on a grid.\n",
    "We want our data to be a $1√ón$ matrix show"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 100\n",
    "x = range(-1, 1; length = n)\n",
    "y = exp.(x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our one-layer neural network (before the summation) is\n",
    "$$\n",
    "  relu.(ùêöx + ùêõ)\n",
    "$$\n",
    "which corresponds to a simple dense layer with `relu` activation.\n",
    "We then sum over the output of this to get the model\n",
    "$$\n",
    "  [1,‚Ä¶,1]^‚ä§ relu.(ùêöx + ùêõ)\n",
    "$$\n",
    "In our case `x` will be a vector containing the grid we sample on\n",
    "but we first need to transpose it to be a $1 √ó n$ matrix, which will apply the NN\n",
    "to each grid point. We can then sum over the columns to get the value of the model with the given\n",
    "paramaters at the grid points."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nn = Dense(1 => n, relu)\n",
    "function summation_model(nn, x, ps)\n",
    "    Y,st = nn(x', ps, NOSTATE) # k-th column contains relu.(ùêöx[k] + ùêõ)\n",
    "    vec(sum(Y; dims=1)) # sums over the columns\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to choose the paramters to minimise a loss function. Here we\n",
    "just wish to minimise the 2-norm erro which we can write as follow:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function convex_regression_loss(nn, ps, (x,y))\n",
    "    yÃÉ = summation_model(nn, x, ps)\n",
    "    norm(yÃÉ - y) # 2-norm error\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could try to use Optimizers.jl directly on this function,\n",
    "but Lux.jl provides a wrapper around the optimisation routine that is convenient for ML.\n",
    "We need to make sure our loss function fits the right interface:\n",
    "in addition to th eloss we also need to return the state and the \"computed statistics\","
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#¬† which in our case is empty.\n",
    "\n",
    "convex_regression_loss_lux(nn, ps, st, (x,y)) = convex_regression_loss(nn, ps, (x,y)), st, ()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now run our optimisation routine. Here we run over 250 \"epochs\": this specifies the number of times\n",
    "we do optimisation. We wrap this in a simple function that we can reuse for other examples:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rng = MersenneTwister()\n",
    "opt = Adam(0.03f0)\n",
    "tstate = Lux.Experimental.TrainState(rng, nn, opt)\n",
    "\n",
    "function train(lossfunc, data, tstate, N)\n",
    "    for epoch in 1:N\n",
    "        grads, loss, stats, tstate = Lux.Training.compute_gradients(AutoZygote(), lossfunc, data, tstate)\n",
    "        if epoch % 50 == 1 || epoch == N\n",
    "            @show epoch, loss\n",
    "        end\n",
    "        tstate = Lux.Training.apply_gradients(tstate, grads)\n",
    "    end\n",
    "    tstate\n",
    "end\n",
    "\n",
    "tstate = train(convex_regression_loss_lux, (x,y), tstate, 250)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compare our approximation to the data and see we have successfully trained the simple neural network:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(x, y)\n",
    "plot!(x, summation_model(nn, x, tstate.parameters))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 2**  Replace `relu` in the activation function with a smooth `tanh` function and plot\n",
    "the result. Is the approximation as accurate? What if you increase the number of epochs?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: setup a neural network with a different activation"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Multiple layer neural networks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An effective neural network will have more than one layer. A simple example is if we want to go beyond\n",
    "convex functions. Rather than simply summing over the neural network we can allow different weights,\n",
    "giving us the model\n",
    "$$\n",
    "  ùêú^‚ä§ relu.(ùêöx + ùêõ) + d.\n",
    "$$\n",
    "Or we can think of $C = ùêú^‚ä§$ as a $1 √ó n$ matrix. This is in fact a composition of two simple layers, the first being\n",
    "$$\n",
    " x ‚Ü¶ relu.(ùêöx + ùêõ)\n",
    "$$\n",
    "and the second being\n",
    "$$\n",
    " ùê± ‚Ü¶ C ùê± + d\n",
    "$$\n",
    "i.e., they are both `Dense` layers just with different dimensions and different activation functions (`relu` and `identity`).\n",
    "We can create such a composition using the `Chain` command:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 100\n",
    "model = Chain(Dense(1 => n, relu), Dense(n => 1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here the parameters are nested. For example, we can create the relevant parameters as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ùêö,ùêõ = randn(n,1),randn(n)\n",
    "ùêú,d = randn(n),randn(1)\n",
    "\n",
    "st = (layer_1 = NOSTATE, layer_2 = NOSTATE) # each layer has its own state\n",
    "ps = (layer_1 = (weight = ùêö, bias = ùêõ), layer_2 = (weight = ùêú', bias = d))\n",
    "\n",
    "@test model([0.1], ps, st)[1] ‚âà ùêú'*relu.(ùêö*0.1 + ùêõ) + d"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can plot the model evaluated at the gri to see that it is indeed (probably) no longer convex:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(x, vec(model(x', ps, st)[1]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now choose the parameters to fit data. Let's generate data for a non-convex function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = range(-1, 1; length = n)\n",
    "y = sin.(3x).*exp.(x)\n",
    "plot(x,y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will fit this data by minimising the 2-norm with a different model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function regression_loss(model, ps, st, (x,y))\n",
    "    yÃÉ = vec(model(x', ps, st)[1])\n",
    "    norm(yÃÉ - y) # 2-norm error\n",
    "end\n",
    "\n",
    "regression_loss_lux(model, ps, st, (x,y)) = regression_loss(model, ps, st, (x,y)), st, ()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now run the optimiser:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "opt = Adam(0.03f0)\n",
    "tstate = Lux.Experimental.TrainState(rng, model, opt)\n",
    "\n",
    "tstate = train(regression_loss_lux, (x,y), tstate, 1000)\n",
    "\n",
    "\n",
    "plot(x,y)\n",
    "plot!(x, vec(model(x', tstate.parameters, tstate.states)[1]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    " It does OK but is still not particularly impressive. The real power in neural networks is their approximation power\n",
    "increases as we add more layers. Here let's try an example with 3-layers."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Chain(Dense(1 => n, relu), Dense(n => n, relu), Dense(n => 1))\n",
    "tstate = Lux.Experimental.TrainState(rng, model, opt)\n",
    "\n",
    "tstate = train(regression_loss_lux, (x,y), tstate, 1000)\n",
    "plot!(x, vec(model(x', tstate.parameters, tstate.states)[1]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 3** Add a 4th layer and 5th layer, but not all involving square matrices, and compare the `relu` and `tanh`\n",
    "activation functions. Can you choose the size of the layers to\n",
    "match the eyeball norm? Hint: the answer might be \"no\" üòÖ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
