{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SciML SANUM2024\n",
    "# Lab 3: Reverse-mode automatic differentiation and Zygote.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the number of unknowns becomes large forward-mode automatic differentiation as\n",
    "implemented in ForwardDiff.jl becomes prohibitively expensive for computing gradients and instead we need to\n",
    "use reverse-mode automatic differentiation: this is best thought of as implementing the chain-rule\n",
    "in an automatic fashion, with a specific choice of multiplying the underlying Jacobians."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Computing gradients is important for solving optimisation problems, which is what ultimately what training a neural network\n",
    "is. Therefore we also look at solving some\n",
    "simple optimissation problems, using Optimsation.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Learning Outcomes**\n",
    "\n",
    "1. Computing gradients and derivatives with Zygote.jl\n",
    "2. Basics of reverse-mode automatic differentiation and pullbacks.\n",
    "3. Forward-mode automatic differentiation via pushforwards.\n",
    "4. Using automatic differentiation for implementing gradient descent.\n",
    "5. Solving optimisation with gradient descent and via Optimsation.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Using Zygote.jl for differentiation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin with a simple demonstration of Zygote.jl, which can be thought of as a replacement for ForwardDiff.jl that\n",
    "uses reverse-mode differentiation under the hood. We can differentiate scalar functions, but unlike ForwardDiff.jl it\n",
    "overloads the `'` syntax to mean differentiation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Zygote, LinearAlgebra, Test\n",
    "\n",
    "\n",
    "@test cos'(0.1) ‚âà -sin(0.1) # Differentiates cos using reverse-mode autodiff"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The real power of Zygote.jl is computing gradients (or more generally, Jacobians\n",
    "of $f : ‚Ñù^m ‚Üí ‚Ñù^n$ where $n ‚â™ m$). We can compute a gradient of the function we considered before as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f = function(x)\n",
    "    ret = zero(eltype(x))\n",
    "    for k = 1:length(x)-1\n",
    "        ret += x[k]*x[k+1]\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "\n",
    "x = randn(5)\n",
    "Zygote.gradient(f,x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unlike ForwardDiff.jl, the gradient returns a tuple since multiple arguments are supported in addition\n",
    "to vector inputs, eg:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x,y = 0.1, 0.2\n",
    "@test all(Zygote.gradient((x,y) -> cos(x*exp(y)), x, y) .‚âà [-sin(x*exp(y))*exp(y), -sin(x*exp(y))*x*exp(y)])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now differentiating this function is not particularly faster than ForwardDiff.jl:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = randn(1000)\n",
    "@time Zygote.gradient(f, x);\n",
    "x = randn(10_000)\n",
    "@time Zygote.gradient(f, x); # roughly 200x slower"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is because not all operations are ameniable to reverse-mode differentiation as implemented in Zygote.jl.\n",
    "However, if we restrict to vectorised operations we see a dramatic improvement:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f_vec = x -> sum(x[1:end-1] .* x[2:end]) # a vectorised version of the previus function\n",
    "Zygote.gradient(f_vec, x) # compile\n",
    "@time Zygote.gradient(f_vec, x); #  1500x faster ü§©"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another catch is Zygote.jl doesn't support functions that mutate arrays. Here's an example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f! = function(x)\n",
    "    n = length(x)\n",
    "    ret = zeros(eltype(x), n)\n",
    "    for k = 1:n-1\n",
    "        ret[k] = x[k]*x[k+1] # modifies the vector ret\n",
    "    end\n",
    "    sum(ret)\n",
    "end\n",
    "\n",
    "\n",
    "x = randn(5)\n",
    "Zygote.gradient(f!,x) # errors out"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is unlike `ForwardDiff.gradient` which works fine for differentiating `f!`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Conclusion**: Zygote.jl is much more brittle, sometimes fails outright, requires\n",
    "writing functions in a specific way, uses a lot more memory to record complicated operations, but when it works\n",
    "well it is _extremely_ fast. Thus when we get to neural networks it is paramount that\n",
    "we design our representation of the neural network in a way that is ameniable to reverse-mode\n",
    "automatic differentiation, as implemented in Zygote.jl."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Pullbacks and back-propagation for scalar functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now peek a little under-the-hood to get some intuition on how Zygote.jl is computing\n",
    "derivatives, and to understand why its so much faster than ForwardDiff.jl in certain situations. Underlying automatic\n",
    "differentiation in Zygote.jl are so-called \"pullback\"s. In the scalar\n",
    "case these are very close to the notion of a derivative. However, rather than\n",
    "the derivative being a single constant, it's a linear map representing the derivative:\n",
    "eg, if the derivative of $f(x)$ is denoted $f'(x)$ then the pullback is a linear map\n",
    "$$\n",
    "t ‚Ü¶ f'(x)t.\n",
    "$$\n",
    "We can compute pullbacks using the `pullback` routine:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "s, p_sin = pullback(sin, 0.1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`p_sin` contains the map $t ‚Ü¶ \\cos(0.1) t$. Since pullbacks support multiple arguments\n",
    "it actually returns a tuple with a single entry:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "p_sin(1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus to get out the value we use the following:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@test p_sin(1)[1] == cos(0.1)\n",
    "@test p_sin(2)[1] == 2cos(0.1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The reason its a map instead of just a scalar becomes important for the vector-valued case\n",
    "where Jacobians can often be applied to vectors much faster than constructing the Jacobian matrix and\n",
    "performing a matrix-vector multiplication."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pullbacks can be used for determining more complicated derivatives. Consider a composition of three functions\n",
    "$h ‚àò g ‚àò f$ where from the Chain Rule we know:\n",
    "$$\n",
    "{{\\rm d} \\over {\\rm d} x}[f(g(h(x))] = f'(g(h(x)) g'(h(x)) h'(x)\n",
    "$$\n",
    "Essentially we have three pullbacks: the first is the pullback of $f$ evaluated\n",
    "at $x$, the second corresponding to $g$ evaluated at $f(x)$, and the third\n",
    "corresponding to $h$ evaluated at $g(f(x))$, that is:\n",
    "$$\n",
    "\\begin{align*}\n",
    " p_1(t) &= f'(x) t  \\\\\n",
    " p_2(t) &= g'(f(x)) t  \\\\\n",
    " p_3(t) &= h'(g(f(x))t\n",
    "\\end{align*}\n",
    "$$\n",
    "Thus the derivative is given by either the _forward_ or _reverse_ composition of these functions:\n",
    "$$\n",
    " p_3(p_2(p_1(1))) = p_1(p_2(p_3(1))) = h'(g(f(x))g'(f(x))f'(x).\n",
    "$$\n",
    "The first version is called _forward-propagation_ and the second called _back-propagation_.\n",
    "Forward-propagation is a version of forward-mode automatic differentiation and is essentially equivalent to using dual numbers.\n",
    "We will see later in the vector case that forward- and back-propagation are not the same,\n",
    "and that back-propagation is much more efficient provided the output is scalar (or small dimensional)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see pullbacks in action for computing the derivative of $\\cos\\sqrt{{\\rm e}^x}$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = 0.1 # point we want to differentiate\n",
    "y,p‚ÇÅ = pullback(exp, x)\n",
    "z,p‚ÇÇ = pullback(sqrt, y) # y is exp(x)\n",
    "w,p‚ÇÉ = pullback(cos, z) # z is sqrt(exp(x))\n",
    "\n",
    "@test w == cos(sqrt(exp(x)))\n",
    "\n",
    "@test p‚ÇÅ(p‚ÇÇ(p‚ÇÉ(1)...)...)[1] ‚âà p‚ÇÉ(p‚ÇÇ(p‚ÇÅ(1)...)...)[1] ‚âà -sin(sqrt(exp(x)))*exp(x)/(2sqrt(exp(x)))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see how this can lead to an approach for automatic differentiation.\n",
    "For example, consider the following function composing `sin` over and over:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function manysin(n, x)\n",
    "    r = x\n",
    "    for k = 1:n\n",
    "        r = sin(r)\n",
    "    end\n",
    "    r\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we would need `n` pullbacks as each time `sin` is called at a different value.\n",
    "But the number of such pullbacks grows only linearly so this is acceptable. So thus\n",
    "at a high-level we can think of Zygote as running through and computing all the pullbacks:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 5\n",
    "x = 0.1 # input\n",
    "\n",
    "pullbacks = Any[] # a vector where we store the pull backs\n",
    "r = x\n",
    "for k = 1:n\n",
    "    r,p‚Çñ = pullback(sin, r) # new pullback\n",
    "    push!(pullbacks, p‚Çñ)\n",
    "end\n",
    "r # value"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To deduce the derivative we need can either do forward- or back-propogation by looping through our pullbacks\n",
    "either in forward- or in reverse-order. Here we implement back-propagation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "reverse_der = 1 # we always initialise with the trivial scaling\n",
    "for k = n:-1:1\n",
    "    reverse_der = pullbacks[k](reverse_der)[1]\n",
    "end\n",
    "@test reverse_der ‚âà (x -> manysin(n, x))'(x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zygote constructs code that is equivalent to this loop automatically,\n",
    "constructing a high-performance version of this back-propogation loop at compile time using something called source-to-source\n",
    "differentiation. But there's no getting around the fact that it needs to record the pullbacks so it does use more memory the larger\n",
    "the computation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Zygote.gradient(manysin, 10, 1.0) # compile\n",
    "@time Zygote.gradient(manysin, 10, 1.0) # uses 4KiB of memory\n",
    "@time Zygote.gradient(manysin, 1000, 1.0) # uses 235KiB of memory\n",
    "@time Zygote.gradient(manysin, 100_000, 1.0) # uses 21MiB of memory"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 1** Compute the derivative of `manysin` using forward-propagation, by looping through the pull-backs\n",
    "in the forward direction."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: loop through pullbacks in order to compute the derivative.\n",
    "# SOLUTION\n",
    "forward_der = 1 # we always initialise with the trivial scaling\n",
    "for k = 1:n\n",
    "    forward_der = pullbacks[k](forward_der)[1]\n",
    "end\n",
    "\n",
    "@test reverse_der ‚âà (x -> manysin(n, x))'(x)\n",
    "\n",
    "# END"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Pullbacks with multiple arguments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Things become more complicated when we have a function with multiple arguments, even in the\n",
    "scalar case. Consider now the function $f(g(x), h(x))$. The chain rule tells us that\n",
    "$$\n",
    "{{\\rm d} \\over {\\rm d} x}[f(g(x), h(x))] = f_x(g(x), h(x)) g'(x) + f_y(g(x), h(x)) h'(x)\n",
    "$$\n",
    "Now we have three pullbacks:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_1(t) &= g'(x) t\\\\\n",
    "p_2(t) &= h'(x) t\\\\\n",
    "p_3(t) &= [f_x(g(x), h(x))t, f_y(g(x), h(x))t]\n",
    "\\end{align*}\n",
    "$$\n",
    "In this case the derivative can be recovered via back-propagation via:\n",
    "$$\n",
    "p_1(p_3(1)[1]) + p_2(p_3(1)[2]).\n",
    "$$\n",
    "Here we see a simple example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f = (x,y) -> cos(x*exp(y))\n",
    "g = sqrt\n",
    "h = sin\n",
    "F = x -> f(g(x), h(x))\n",
    "\n",
    "x = 0.1\n",
    "gx, p‚ÇÅ = pullback(g, x)\n",
    "hx, p‚ÇÇ = pullback(h, x)\n",
    "z, p‚ÇÉ = pullback(f, gx, hx)\n",
    "\n",
    "@test p‚ÇÅ(p‚ÇÉ(1)[1])[1] + p‚ÇÇ(p‚ÇÉ(1)[2])[1] ‚âà F'(0.1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Doing more complicated calculations or indeed algorithms becomes\n",
    "quite complicated if there are interdependencecies, eg, $f(g(r(x)), h(r(x)))$.\n",
    "Fortunately, there is an alternative: we can focus on composing vector functions.\n",
    "Eg, such a function can be thought of as composition:\n",
    "$$\n",
    "f ‚àò ùê† ‚àò r\n",
    "$$\n",
    "where $ùê†(x) = [g(x),h(x)]$. This is a special case of what we discuss in the next section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Gradients and pullbacks\n",
    "\n",
    "Now we consider computing gradients of functions that are compositions\n",
    "of vector functions, which neural networks fall into.\n",
    "Again, we denote the Jacobian as\n",
    "$$\n",
    " J_f = \\begin{bmatrix} {‚àÇ f_1 \\over ‚àÇx_1} & ‚ãØ & {‚àÇ f_1 \\over ‚àÇx_‚Ñì} \\\\\n",
    "      ‚ãÆ & ‚ã± & ‚ãÆ \\\\\n",
    "      {‚àÇ f_m \\over ‚àÇx_1} & ‚ãØ & {‚àÇ f_m \\over ‚àÇx_‚Ñì}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that gradients are the transpose of Jacobians: $‚àáh = J_h^‚ä§$.\n",
    "For a scalar-valued function $f : ‚Ñù^n ‚Üí ‚Ñù$ the pullback represents the linear map\n",
    "$p_{f,ùê±} : ‚Ñù ‚Üí ‚Ñù^n$ corresponding to scaling the gradient:\n",
    "$$\n",
    "p_{f,ùê±}(t) = J_f(ùê±)^‚ä§t = ‚àáf(ùê±) t\n",
    "$$\n",
    "Here we see an example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f = (ùê±) -> ((x,y) = ùê±;  exp(x*cos(y)))\n",
    "x,y = (0.1,0.2)\n",
    "f_v, f_pb = Zygote.pullback(f, [x,y])\n",
    "@test f_pb(1)[1] ‚âà [exp(x*cos(y))*cos(y), -exp(x*cos(y))*x*sin(y)]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a function $f : ‚Ñù^n ‚Üí ‚Ñù^m$ the the pullback represents the linear map $p_{f,ùê±} : ‚Ñù^m ‚Üí ‚Ñù^n$ given by\n",
    "$$\n",
    "p_{f,ùê±}(t) = J_f(ùê±)^‚ä§ùê≠\n",
    "$$\n",
    "Here is a simple example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f = function(ùê±)\n",
    "    x,y,z = ùê±\n",
    "    [exp(x*y*z),cos(x*y+z)]\n",
    "end\n",
    "\n",
    "\n",
    "ùê± = [0.1,0.2,0.3]\n",
    "f_x, p_f =  pullback(f, ùê±) # returns the value and pullback\n",
    "\n",
    "J_f = function(ùê±)\n",
    "    x,y,z = ùê±\n",
    "    [y*z*exp(x*y*z) x*z*exp(x*y*z) x*y*exp(x*y*z);\n",
    "     -y*sin(x*y+z) -x*sin(x*y+z) -sin(x*y+z)]\n",
    "end\n",
    "\n",
    "ùê≤ = [1,2]\n",
    "@test J_f(ùê±)'*ùê≤ ‚âà p_f(ùê≤)[1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider a composition $f : ‚Ñù^n ‚Üí ‚Ñù^m$, $g : ‚Ñù^m ‚Üí ‚Ñù^‚Ñì$ and $h : ‚Ñù^‚Ñì ‚Üí ‚Ñù$, that is,\n",
    "we want to compute the gradient of $h ‚àò g ‚àò f : ‚Ñù^n ‚Üí ‚Ñù$. The Chain rule tells us that\n",
    "$$\n",
    " J_{h ‚àò g ‚àò f}(ùê±) = J_h(g(f(ùê±)) J_g(f(ùê±)) J_f(ùê±)\n",
    "$$\n",
    "Put another way, the gradiant of $h ‚àò g ‚àò f$\n",
    "is given by the transposes of Jacobians:\n",
    "$$\n",
    "   ‚àá[{h ‚àò g ‚àò f}](ùê±) = J_f(ùê±)^‚ä§ J_g(f(ùê±))^‚ä§  ‚àáh(g(f(ùê±))\n",
    "$$\n",
    "Thus we have three pullbacks $p_1 : ‚Ñù^m ‚Üí ‚Ñù^n$, $p_2 : ‚Ñù^‚Ñì ‚Üí ‚Ñù^m$ and $p_3 : ‚Ñù ‚Üí ‚Ñù^‚Ñì$ given by\n",
    "\\begin{align*}\n",
    " p_1(ùê≠) &= J_f(ùê±)^‚ä§ ùê≠  \\\\\n",
    " p_2(ùê≠) &= J_g(f(x))^‚ä§ ùê≠  \\\\\n",
    " p_3(t) &= ‚àáh(g(f(ùê±)) t\n",
    "\\end{align*}\n",
    "The gradient is given by _back-propagation_:\n",
    "$$\n",
    " p_1(p_2(p_3(1))) = J_f(ùê±)^‚ä§ J_g(f(ùê±))^‚ä§  ‚àáh(g(f(ùê±)).\n",
    "$$\n",
    "Here the \"right\" order to do the multiplications is clear: matrix-matrix multiplications are expensive\n",
    "so its best to do it reverse order so that we only ever have matrix-vector multiplications.\n",
    "Also, the pullback doesn't give us enough information to implement forward-propagation:\n",
    "we don't have access to the Jacobian matrices, or their application."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an example consider computing the gradient of an iteration a simple map like:\n",
    "$$\n",
    "ùêü(x,y,z) = \\begin{bmatrix} \\cos(xy)+z\\\\ zy-\\exp(x)\\\\ x + y + z \\end{bmatrix}\n",
    "$$\n",
    "and summing over the result, eg. computing $[1,1,1]^‚ä§(\\underbrace{ùêü ‚àò ‚ãØ ‚àò ùêü}_{n\\hbox{ times}})(ùê±)$.\n",
    "We implement this with a general function `iteratef`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ùêü = function(ùê±)\n",
    "    (x,y,z) = ùê±\n",
    "    [cos(x*y)+z, z*y-sin(x), x + y + z]\n",
    "end\n",
    "\n",
    "function iteratef(ùê±, ùêü, n)\n",
    "    for k = 1:n\n",
    "        ùê± = ùêü(ùê±)\n",
    "    end\n",
    "    sum(ùê±)\n",
    "end\n",
    "\n",
    "gradient(iteratef, [0.1,0.2,0.3] , ùêü, 5)[1] # computes the gradient of 5 iterations"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get an idea how this works behind the scenes we can again accumulate the pullbacks:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pullbacks = Any[] # a vector where we store the pull backs\n",
    "r = [0.1,0.2, 0.3]\n",
    "n = 5\n",
    "for k = 1:n\n",
    "    r,p‚Çñ = pullback(ùêü, r) # new pullback\n",
    "    push!(pullbacks, p‚Çñ)\n",
    "end\n",
    "\n",
    "ret,sumpullback = pullback(sum, r)\n",
    "ret # value"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can recover the gradient by back-propogation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "reverse_grad = 1\n",
    "reverse_grad = sumpullback(reverse_grad)[1] # now a 3-vector\n",
    "for k = n:-1:1\n",
    "    reverse_grad = pullbacks[k](reverse_grad)[1]\n",
    "end\n",
    "reverse_grad"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indeed we match the gradient as computed with Zygote.jl:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@test reverse_grad == gradient(iteratef, [0.1,0.2,0.3] , ùêü, n)[1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 2** The function `pushforward` represent the map $ùê≠ ‚Ü¶ J_f(ùê±) ùê≠$.\n",
    "Compute the gradient of `iteratef` as above with forward-mode automatic differentiation by using `pushforward`.\n",
    "Do so without creating a vector of pushforwards.\n",
    "Hint: We need to run the pushforward iteration with the identity matrix as the initial value,\n",
    "but the result of  `pushforward` only works on vectors. So we need to apply it to each column of the matrix manually."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: Compute the gradient as above but using pushforward\n",
    "# SOLUTION\n",
    "\n",
    "r = [0.1,0.2, 0.3]\n",
    "X = Matrix(1.0I, 3, 3)\n",
    "n = 5\n",
    "for k = 1:n\n",
    "    p‚Çñ = pushforward(ùêü, r) # new pushforward\n",
    "    for j = 1:3\n",
    "        X[:,j] = p‚Çñ(X[:,j])\n",
    "    end\n",
    "    r = ùêü(r)\n",
    "end\n",
    "sumpushforward = pushforward(sum, r)\n",
    "\n",
    "grad = [sumpushforward(X[:,j]) for j = 1:3]\n",
    "\n",
    "\n",
    "@test grad ‚âà gradient(iteratef, [0.1,0.2,0.3] , ùêü, n)[1]\n",
    "\n",
    "# END"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 3** Consider a simple forward Euler method approximating the solution to the Pendulum equation with friction:\n",
    "$$\n",
    "u'' = œÑ u' - \\sin u\n",
    "$$\n",
    "which we can rewrite as a first order system:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   u' \\\\\n",
    "   v'\n",
    "   \\end{bmatrix} = \\begin{bmatrix} v \\\\ -œÑ*v - \\sin u \\end{bmatrix}\n",
    "$$\n",
    "That is, we want to implement the iteration\n",
    "$$\n",
    "ùêÆ_{k+1} = ùêÆ_k + h*\\begin{bmatrix} ùêÆ_k[2] \\\\ -œÑ ùêÆ_k[2] - \\sin ùêÆ_k[1] \\end{bmatrix}\n",
    "$$\n",
    "with a specified initial condition $ùêÆ_0$. For $N = 100$, $h = 0.1$ and $ùêÆ_0 = [0.1,0.2]$, differentiate\n",
    "the solution with-respect to $œÑ$ at $œÑ = 1$ by creating a vector of pullbacks and implementing back-propagation.\n",
    "Hint: Forward Euler is a variant of `iteratef` above so you can modify the subsequent pullback construction. Add $œÑ$ to the vector\n",
    "of values to capture the relevant dependencies and verify your result by comparing to `gradient`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# SOLUTION\n",
    "ùêü = function(h, ùê±)\n",
    "    (œÑ,u,v) = ùê±\n",
    "    [œÑ,u + h*v, v + h*(-œÑ*v - sin(u))]\n",
    "end\n",
    "\n",
    "function forwardeuler(œÑ, ùêÆ‚ÇÄ, ùêü, h, n)\n",
    "    ùê± = [œÑ; ùêÆ‚ÇÄ]\n",
    "    for k = 1:n\n",
    "        ùê± = ùêü(h, ùê±)\n",
    "    end\n",
    "    ùê±[2]\n",
    "end\n",
    "\n",
    "\n",
    "forwardeuler(1.0,[0.1,0.2], ùêü, 0.1, 100)\n",
    "\n",
    "pullbacks = Any[] # a vector where we store the pull backs\n",
    "ùê± = [1.0,0.1, 0.2]\n",
    "n = 100\n",
    "h = 0.1\n",
    "for k = 1:n\n",
    "    ùê±,p‚Çñ = pullback(ùêü, h, ùê±) # new pullback\n",
    "    push!(pullbacks, p‚Çñ)\n",
    "end\n",
    "\n",
    "ret,firstpullback = pullback(getindex, ùê±, 2)\n",
    "ret # value\n",
    "\n",
    "# We can recover the gradient by back-propogation:\n",
    "\n",
    "reverse_grad = 1\n",
    "reverse_grad = firstpullback(reverse_grad)[1] # now a 3-vector\n",
    "for k = n:-1:1\n",
    "    reverse_grad = pullbacks[k](reverse_grad)[2]\n",
    "end\n",
    "@test reverse_grad[1] ‚âà gradient(forwardeuler, 1.0, [0.1,0.2], ùêü, 0.1, 100)[1]\n",
    "\n",
    "\n",
    "\n",
    "# END"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 Optimisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A key place where reverse-mode automatic differentiation is essential is large scale optimisation.\n",
    "As a  simple example we will look at the classic optimisation problem\n",
    "that solves $A ùê± = ùêõ$ where $A$ is symmetric positive definite: find $ùê±$ that minimises\n",
    "$$\n",
    "f_{A,ùêõ}(ùê±) = ùê±^‚ä§ A ùê± - 2ùê±^‚ä§ ùêõ.\n",
    "$$.\n",
    "Of course we can use tried-and-true techniques implemented in `\\` but here we want\n",
    "to emphasise we can also solve this with simple optimsation algorithms like gradient desecent\n",
    "which do not know the structure of the problem. We consider a matrix where we know gradient descent\n",
    "will converge fast:\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 1/2^Œ± \\\\ 1/2^Œ± & 1 & ‚ã± \\\\ &  ‚ã± & ‚ã± & 1/n^Œ± \\\\ && 1/n^Œ± & 1 \\end{bmatrix}\n",
    "$$\n",
    "In other words we want to minimise the functional (or the _loss function_)\n",
    "$$\n",
    "f_{A,ùêõ}(ùê±) = ‚àë_{k=1}^n x_k^2 + ‚àë_{k=2}^n x_{k-1} x_k/k^Œ± - ‚àë_{k=1}^n x_k b_k.\n",
    "$$\n",
    "For simplicity we will take $ùêõ$ to be the vector with all ones."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Owing to the constraints of Zygote.jl, we need to write this in a vectorised way to ensure Zygote is sufficiently fast.\n",
    "Here we see that when we do this we can efficiently\n",
    "compute gradients even\n",
    "with a million degrees of freedom, way beyond what could ever be done with forward-mode automatic differentiation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 1_000_000\n",
    "f = (x,Œ±) -> (x'x + 2x[1:end-1]'*(x[2:end] ./ (2:length(x)).^Œ±)) - 2sum(x)\n",
    "\n",
    "\n",
    "\n",
    "x = randn(n) # initial guess\n",
    "Zygote.gradient(f, x, 2) # compile\n",
    "@time Zygote.gradient(f, x, 2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For concreteness we first implement our own version of a quick-and-dirty gradient descent:\n",
    "$$\n",
    "x_{k+1} = x_k - Œ≥_k ‚àáf(x_k)\n",
    "$$\n",
    "where $Œ≥_k$ is the learning rate. To choose $Œ≥_k$ we just halve\n",
    "the learning rate until we see decrease in the loss function."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Œ± = 2\n",
    "for k = 1:20\n",
    "    Œ≥ = 1\n",
    "    y = x - Œ≥*Zygote.gradient(f, x, Œ±)[1]\n",
    "    while f(x,Œ±) < f(y,Œ±)\n",
    "        Œ≥ /= 2 # half the learning rate\n",
    "        y = x - Œ≥*Zygote.gradient(f, x, Œ±)[1]\n",
    "    end\n",
    "    x = y\n",
    "    @show Œ≥,f(x,Œ±)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compare this with the \"true\" solution:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "A = SymTridiagonal(ones(n), (2:n) .^ (-2))\n",
    "@test x ‚âà A\\ones(n)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In practice its better to use inbuilt optimsation routines and packages. Here we see how we can solve the same problem with\n",
    "the Optimization.jl package, combined with OptimizationOptimisers.jl that has gradient-based optimisation methods,\n",
    "in particular `Adam`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Optimization, OptimizationOptimisers\n",
    "\n",
    "x = randn(n) # initial guess\n",
    "prob = OptimizationProblem(OptimizationFunction(f, Optimization.AutoZygote()), x, n)\n",
    "@time y = solve(prob, Adam(0.03), maxiters=100)\n",
    "\n",
    "@test y.u ‚âà x"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 4** This problem considers an example that will connect with  neural networks.\n",
    "Define ${\\rm relu}(x) := \\max(0,x)$ and consider an approximation of the form:\n",
    "$$\n",
    "p_{ùêö,ùêõ}(x) := ‚àë_{k=1}^n {\\rm relu}(a_k x + b_k)\n",
    "$$\n",
    "where $ùêö,ùêõ ‚àà ‚Ñù^n$. This is a sum of positive convex functions hence consider regression for a positive convex function\n",
    "like $f(x) =  \\exp x$. For $n = 100$,  approximate $ùêö,ùêõ$ that minimises $\\|p_{ùêö,ùêõ}.(ùê±) - f.(ùê±)\\|$ where $ùê±$ is a vector containing\n",
    "100 evenly spaced points between $-1$ and $1$ (inclusive). Compare your regression with $f$ by plotting the two functions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# TODO: Construct a model for the function and perform regression using Optimization.jl\n",
    "# SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "n = 100\n",
    "x = range(-1, 1; length = n)\n",
    "y = exp.(x)\n",
    "\n",
    "relu(x) = max(0,x)\n",
    "\n",
    "# Make a function that implements the sum\n",
    "function summation_model(x, ùêöùêõ)\n",
    "    n = length(ùêöùêõ) √∑ 2\n",
    "    (ùêö, ùêõ) = ùêöùêõ[1:n], ùêöùêõ[n+1:end]\n",
    "    Y = relu.(ùêö*x' .+ ùêõ)\n",
    "    vec(sum(Y; dims=1)) # sums over the columns\n",
    "end\n",
    "\n",
    "\n",
    "# Our loss function takes in x and y as parameters\n",
    "convex_regression_loss(ùêöùêõ, (x,y)) = norm(summation_model(x, ùêöùêõ) - y)\n",
    "\n",
    "\n",
    "ùêö,ùêõ = randn(n),randn(n)\n",
    "prob = OptimizationProblem(OptimizationFunction(convex_regression_loss, Optimization.AutoZygote()), [ùêö;ùêõ], (x,y))\n",
    "@time ret = solve(prob, Adam(0.03), maxiters=1000)\n",
    "\n",
    "\n",
    "using Plots\n",
    "plot(x, y)\n",
    "plot!(x, summation_model(x, ret.u))\n",
    "\n",
    "# END"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
