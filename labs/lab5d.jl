# # SciML SANUM2024
# # Lab 5: Neural Networks and Lux.jl
#
# In this lab we introduce neural networks as implemented in Lux.jl. 
# A neural network (NN) is in some sense just a function with many parameters
# in a way that facilitates computing gradients with respect to these parameters.
# That is: it is at its core a way of book-keeping a heavily parameterised function.
# It is constructed by composing basic building blocks usually built from linear
# algebra operations, combined with simple _activator functions_. 
# Here we look at the simplest case and see how the paremeters in a NN can be chosen to
# solve optimisation problems. In other words: we will _train_ the NN.

# **Learning Outcomes**
# 1. Single-layer neural networks and activation functions.
# 2. Creating deeper networks as a `Chain`.
# 3. Training neural networks by simple optimisation.

using Lux, Random, Optimization, OptimizationOptimisers, ComponentArrays, Zygote, Plots, LinearAlgebra, Test


# ## 4.1 Single layer neural networks

# We begin with a single-layer NN without an activator
# function which correspond to maps of the form:
# $$
# ğ± â†¦ Ağ± + ğ›
# $$
# where $A âˆˆ â„^{m Ã— n}$ and $ğ› âˆˆ â„^n$. The space of such maps is
# modelled by the `Dense` type which has two paramters: `weight`, corresponding to $A$, and
#  `bias`, corresponding to $ğ›$. Here we see a simple example
# of constructing the model (the space of all such maps) and evaluating
# a specific map by specifying the parameters.

# Note we have to pass an extra argument corresponding to
# the "state" of a NN: this doesn't exist for the simple layers we consider
# but more sophisticated NNs can depend on history and so the state records the relevant
# information.


##


# An important feature is that we can compute gradients with respect to parameters of functions of our
# model. Before we looked at the case where
# we differentiated with respect to vectors but a powerful feature in Zygote is it works for other types, including the `NamedTuple`
# which Lux.jl uses for representing paramaters:

##

# Because our NN at this stage is linear in the paremeters the gradient is actually quite simple: eg the partial derivative with
# respect to $A[k,j]$ will just be $x[j]$ and the derivative with respect to $b[k]$ will just be $1$. Thus we get:

##



# Going beyond basic linear algebra, we can apply an "activator" function $f$ to each
# entry of the map, to represent maps of the form:
# $$
# ğ± â†¦ f.(Ağ± + ğ›)
# $$
# where we use the Julia-like broadcast notation to mean entrywise application.
# The classic activator is the `relu` function which is really just $\max(0,x)$:

##

# We can incorporate this in our model as follows:

##

# And we can compute gradients as before:

##

# **Problem 1** Derive the formula  for the gradient of the model with an activator function and compare it with
# the numerical result just computed. Hint: The answer depends on the output value.

##Â TODO: Compute the gradient by hand, matching ps_grad


# Let's see an example directly related to a classic numerical analysis problem: approximating 
# functions by a continuous piecewise affine
# function, as done in the Trapezium rule. Our model corresponds to a sum of weighted and shifted `relu` functions:
# $$
# p_{ğš,ğ›}(x) := âˆ‘_{k=1}^n {\rm relu}(a_k x + b_k)
# $$
# We note that this is a sum of positive convex functions so will only be useful for approximating positive convex functions
# (we will generalise this later).  Thus we want to choose the paremeters to fit data generated by a positive convex function,
# e.g., $f(x) = \exp(x)$. Here we first generate "training data" which means the samples of the function on a grid.

##

# Our one-layer NN (before the summation) is
# $$
#   {\rm relu}.(ğšx + ğ›)
# $$
# which corresponds to a simple dense layer with `relu` activation.
# We then sum over the output of this to get the model
# $$
#   [1,â€¦,1]^âŠ¤ {\rm relu}.(ğšx + ğ›)
# $$
# In our case `x` is actually a vector containing the grid we sample on
# but we first need to transpose it to be a $1 Ã— n$ matrix, which will apply the NN
# to each grid point. We can then sum over the columns to get the value of the model with the given
# parameters at the grid points.

##

# We want to choose the parameters to minimise a loss function. Here we
# just wish to minimise the 2-norm error which we can write as follow:

##


# We now setup the optimation problem. We can use `Lux.setup` to create a random initial guess for parameters
# though we need to supply a random number generator. We also need to wrap the returned named tuple in a `ComponentArray` as Optimization.jl
# requires the optimisation to be over an array-type.

##


# **Problem 2**  Replace `relu` in the activation function with a smooth `tanh` function and plot
# the result. Is the approximation as accurate? What if you increase the number of epochs?
# What if you construct your own function that is a smooth approximation to `relu`?

## TODO: setup a neural network with different activations







# ## 4.2 Multiple layer neural networks

# An effective NN will have more than one layer. A simple example is if we want to go beyond
# convex functions. Rather than simply summing over the NN we can allow different weights,
# giving us the model
# $$
#   ğœ^âŠ¤ {\rm relu}.(ğšx + ğ›) + d.
# $$
# Or we can think of $C = ğœ^âŠ¤$ as a $1 Ã— n$ matrix. This is in fact a composition of two simple layers, the first being
# $$
#  x â†¦ {\rm relu}.(ğšx + ğ›)
# $$
# and the second being one without an activation function:
# $$
#  ğ± â†¦ C ğ± + d.
# $$
# I.e., they are both `Dense` layers just with different dimensions and different activation functions (`relu` and `identity`).
# We can create such a composition using the `Chain` command:

##

# Here the parameters are nested. For example, we can create the relevant parameters as follows:

##

# We can plot the model evaluated at the gri to see that it is indeed (probably) no longer convex:

##

# We now choose the parameters to fit data. Let's generate data for a non-convex function:

##

# We will fit this data by minimising the 2-norm with a different model:

##


#  It does OK but is still not particularly impressive. The real power in neural networks is their approximation power
# increases as we add more layers. Here let's try an example with 3-layers.

##


# **Problem 3** Add a 4th layer and 5th layer, but not all involving square matrices. 
# Can you choose the size of the layers and the activation functions to
# match the eyeball norm? Hint: the answer might be "no" ğŸ˜… But maybe "ballpark norm" is sufficient.
## TODO: Setup a NN with 4 and 5 layers.
